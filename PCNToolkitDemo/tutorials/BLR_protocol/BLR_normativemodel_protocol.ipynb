{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Andjelaaaa/dimitrijevic_project/blob/main/PCNToolkitDemo/tutorials/BLR_protocol/BLR_normativemodel_protocol.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D68ks6mpPxQV"
      },
      "source": [
        "# [Predictive Clinical Neuroscience Toolkit](https://github.com/amarquand/PCNtoolkit) \n",
        "# The Normative Modeling Framework for Computational Psychiatry Protocol\n",
        "## Using Bayesian Linear Regression and Multi-Site Cortical Thickness Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sekFShu5PxQY",
        "jp-MarkdownHeadingCollapsed": true,
        "tags": []
      },
      "source": [
        "Created by [Saige Rutherford](https://twitter.com/being_saige) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-F8j2e7ePxQY"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/Figure2.png?raw=1\" width=\"1000\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prEV5dymPxQZ"
      },
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuzKCq4SPxQZ"
      },
      "source": [
        "## Install necessary libraries & grab data files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORn68UDCPxQZ"
      },
      "source": [
        "### Step 1.\n",
        "\n",
        "Begin by cloning the GitHub repository using the following commands. This repository contains the necessary code and example data. Then install the python packages using pip and import them into the python environment (either Google Colab or using a local python installation on your computer)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Important info\n",
        "\n",
        "Do not run this cell if this tutorial is ran locally! Only needed if ran directly on Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 175,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XY_jbksGPxQa",
        "outputId": "90454015-ce89-4059-f58d-60748871aef9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'PCNtoolkit-demo'...\n",
            "remote: Enumerating objects: 895, done.\u001b[K\n",
            "remote: Counting objects: 100% (18/18), done.\u001b[K\n",
            "remote: Compressing objects: 100% (18/18), done.\u001b[K\n",
            "remote: Total 895 (delta 2), reused 0 (delta 0), pack-reused 877\u001b[K\n",
            "Receiving objects: 100% (895/895), 18.45 MiB | 2.60 MiB/s, done.\n",
            "Resolving deltas: 100% (364/364), done.\n"
          ]
        }
      ],
      "source": [
        "# Not necessary to run if on a local computer!\n",
        "! git clone https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 114,
      "metadata": {
        "id": "XUqJ0xDTPxQb"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# set this path to the git cloned PCNtoolkit-demo repository --> Uncomment whichever line you need for either running on your own computer or on Google Colab.\n",
        "os.chdir('/home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/tutorials/BLR_protocol/') # if running on your own computer, use this line (change the path to match where you cloned the repository)\n",
        "# os.chdir('/content/PCNtoolkit-demo/tutorials/BLR_protocol') # if running on Google Colab, use this line"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 115,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "z05J1GqFPxQb",
        "jupyter": {
          "outputs_hidden": false
        },
        "outputId": "0faa5a0f-61d4-4c5b-b9ab-1135395ca4fe",
        "pycharm": {
          "name": "#%%\n"
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 1)) (1.4.3)\n",
            "Requirement already satisfied: numpy in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 2)) (1.19.5)\n",
            "Requirement already satisfied: matplotlib in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 3)) (3.3.4)\n",
            "Requirement already satisfied: sklearn in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 4)) (0.0)\n",
            "Requirement already satisfied: joypy in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 5)) (0.2.6)\n",
            "Requirement already satisfied: pcntoolkit in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from -r requirements.txt (line 6)) (0.24)\n",
            "Requirement already satisfied: pytz>=2020.1 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2022.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /home/adimitrijevic/.local/lib/python3.9/site-packages (from pandas->-r requirements.txt (line 1)) (2.8.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (9.0.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /home/adimitrijevic/.local/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (3.0.9)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from matplotlib->-r requirements.txt (line 3)) (1.4.2)\n",
            "Requirement already satisfied: scikit-learn in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sklearn->-r requirements.txt (line 4)) (0.24.2)\n",
            "Requirement already satisfied: scipy>=0.11.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from joypy->-r requirements.txt (line 5)) (1.7.3)\n",
            "Requirement already satisfied: arviz==0.11.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pcntoolkit->-r requirements.txt (line 6)) (0.11.0)\n",
            "Requirement already satisfied: pymc3<=3.9.3,>=3.8 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pcntoolkit->-r requirements.txt (line 6)) (3.9.3)\n",
            "Requirement already satisfied: six in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pcntoolkit->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: bspline in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pcntoolkit->-r requirements.txt (line 6)) (0.1.1)\n",
            "Requirement already satisfied: sphinx-tabs in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pcntoolkit->-r requirements.txt (line 6)) (3.4.1)\n",
            "Requirement already satisfied: theano==1.0.5 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pcntoolkit->-r requirements.txt (line 6)) (1.0.5)\n",
            "Collecting argparse\n",
            "  Using cached argparse-1.4.0-py2.py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: nibabel>=2.5.1 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pcntoolkit->-r requirements.txt (line 6)) (4.0.1)\n",
            "Requirement already satisfied: torch>=1.1.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pcntoolkit->-r requirements.txt (line 6)) (1.11.0)\n",
            "Requirement already satisfied: setuptools>=38.4 in /home/adimitrijevic/.local/lib/python3.9/site-packages (from arviz==0.11.0->pcntoolkit->-r requirements.txt (line 6)) (63.1.0)\n",
            "Requirement already satisfied: packaging in /home/adimitrijevic/.local/lib/python3.9/site-packages (from arviz==0.11.0->pcntoolkit->-r requirements.txt (line 6)) (21.3)\n",
            "Requirement already satisfied: netcdf4 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from arviz==0.11.0->pcntoolkit->-r requirements.txt (line 6)) (1.6.0)\n",
            "Requirement already satisfied: typing-extensions<4,>=3.7.4.3 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from arviz==0.11.0->pcntoolkit->-r requirements.txt (line 6)) (3.10.0.2)\n",
            "Requirement already satisfied: xarray>=0.16.1 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from arviz==0.11.0->pcntoolkit->-r requirements.txt (line 6)) (2022.6.0)\n",
            "Requirement already satisfied: h5py>=2.7.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit->-r requirements.txt (line 6)) (3.1.0)\n",
            "Requirement already satisfied: patsy>=0.5.1 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit->-r requirements.txt (line 6)) (0.5.2)\n",
            "Requirement already satisfied: fastprogress>=0.2.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from pymc3<=3.9.3,>=3.8->pcntoolkit->-r requirements.txt (line 6)) (1.0.3)\n",
            "Requirement already satisfied: cftime in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from netcdf4->arviz==0.11.0->pcntoolkit->-r requirements.txt (line 6)) (1.6.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from scikit-learn->sklearn->-r requirements.txt (line 4)) (2.2.0)\n",
            "Requirement already satisfied: docutils~=0.18.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (0.18.1)\n",
            "Requirement already satisfied: pygments in /home/adimitrijevic/.local/lib/python3.9/site-packages (from sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (2.12.0)\n",
            "Requirement already satisfied: sphinx in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (5.1.0)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (1.1.5)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (2.2.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (4.11.3)\n",
            "Requirement already satisfied: Jinja2>=2.3 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (3.0.3)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (0.7.12)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (1.0.3)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (1.0.2)\n",
            "Requirement already satisfied: imagesize in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (1.4.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (2.0.0)\n",
            "Requirement already satisfied: babel>=1.3 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (2.9.1)\n",
            "Requirement already satisfied: requests>=2.5.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (2.27.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from importlib-metadata>=4.4->sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (3.8.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from Jinja2>=2.3->sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (2.1.1)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from requests>=2.5.0->sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from requests>=2.5.0->sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (3.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from requests>=2.5.0->sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (2022.6.15)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/adimitrijevic/miniconda3/lib/python3.9/site-packages (from requests>=2.5.0->sphinx->sphinx-tabs->pcntoolkit->-r requirements.txt (line 6)) (1.26.8)\n",
            "Installing collected packages: argparse\n",
            "Successfully installed argparse-1.4.0\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wDmki54PxQc"
      },
      "source": [
        "## Prepare covariate data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCEK3IokPxQc"
      },
      "source": [
        "### Step 2.\n",
        "\n",
        "The data set (downloaded in Step 1) includes a multi-site dataset from the [Human Connectome Project Young Adult study](https://www.humanconnectome.org/study/hcp-young-adult) and [IXI](https://brain-development.org/ixi-dataset/). It is also possible to use different datasets (i.e., your own data or additional public datasets) in this step. If using your own data here, it is recommended to load the example data to view the column names in order to match your data to this format. Read in the data files using pandas, then merge the covariate (age & sex) data from each site into a single data frame (named cov). The columns of this covariate data frame represent the predictor variables. Additional columns may be added here, depending on the research question."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdyMs_ovt-no"
      },
      "source": [
        "## NOTE\n",
        "- The initial python version imbedded in Google Colab is 3.7.13, which makes imports from the pcntoolkit not work. This can be much easily bypassed if packages are installed and ran locally instead of on Google colab.\n",
        "- If ran directly on GoogleColab, imports should all work after installing python 3.9.1 (which does not work every time; see previous versions of this jupyter notebook on github).\n",
        "- The problematic import is `from pcntoolkit.normative import estimate, evaluate `\n",
        "- The image below shows the previous error obtained when no python installation is done prior to importing packages\n",
        "\n",
        "<div>\n",
        "<p style=\"text-align:center;\"><img src=\"https://github.com/Andjelaaaa/dimitrijevic_project/blob/main/PCNToolkitDemo/tutorials/BLR_protocol/ImportsProof.png?raw=1\"  width=\"1000\"  ></p>\n",
        "</div>  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 116,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        },
        "id": "sWcyf4OLnrs4",
        "outputId": "16e2b102-808c-42f9-cda4-87ca94612b72"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joypy\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 117,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pcntoolkit.normative import estimate, evaluate\n",
        "from pcntoolkit.util.utils import create_bspline_basis, compute_MSLL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vVAzRfsmz7v"
      },
      "source": [
        "### Additional notes on the code cell above\n",
        "- Does not work directly on Google colab because of the python version [see this link for more info](https://github.com/amarquand/PCNtoolkit/issues/90)\n",
        "- Solution = installing python 3.9.1 as shown in the [previous versions of this jupyter notebook](https://github.com/Andjelaaaa/dimitrijevic_project/commits/main/PCNToolkitDemo/tutorials/BLR_protocol/BLR_normativemodel_protocol.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "id": "G6VQMkq-PxQd"
      },
      "outputs": [],
      "source": [
        "# if running in Google colab, remove the \"data/\" folder from the path\n",
        "hcp = pd.read_csv('/home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/data/HCP1200_age_gender.csv')\n",
        "ixi = pd.read_csv('/home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/data/IXI_age_gender.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OgM-nnT6PxQd",
        "outputId": "fc43a056-b3fa-49c9-e3be-1e9a3e1de6c7"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adimitrijevic/miniconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1219: UserWarning: You are merging on int and float columns where the float values are not equal to their int representation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant_id</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "      <th>site</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sub-100004</td>\n",
              "      <td>24.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>hcp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sub-100206</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>hcp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sub-100307</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>2</td>\n",
              "      <td>hcp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sub-100408</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>hcp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sub-100610</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>1</td>\n",
              "      <td>hcp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1791</th>\n",
              "      <td>sub-IXI648</td>\n",
              "      <td>47.723477</td>\n",
              "      <td>1</td>\n",
              "      <td>ixi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1792</th>\n",
              "      <td>sub-IXI651</td>\n",
              "      <td>50.395619</td>\n",
              "      <td>1</td>\n",
              "      <td>ixi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1793</th>\n",
              "      <td>sub-IXI652</td>\n",
              "      <td>42.989733</td>\n",
              "      <td>1</td>\n",
              "      <td>ixi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1794</th>\n",
              "      <td>sub-IXI653</td>\n",
              "      <td>46.220397</td>\n",
              "      <td>1</td>\n",
              "      <td>ixi</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1795</th>\n",
              "      <td>sub-IXI662</td>\n",
              "      <td>41.741273</td>\n",
              "      <td>1</td>\n",
              "      <td>ixi</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1796 rows × 4 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     participant_id        age  sex site\n",
              "0        sub-100004  24.000000    1  hcp\n",
              "1        sub-100206  27.000000    1  hcp\n",
              "2        sub-100307  27.000000    2  hcp\n",
              "3        sub-100408  33.000000    1  hcp\n",
              "4        sub-100610  27.000000    1  hcp\n",
              "...             ...        ...  ...  ...\n",
              "1791     sub-IXI648  47.723477    1  ixi\n",
              "1792     sub-IXI651  50.395619    1  ixi\n",
              "1793     sub-IXI652  42.989733    1  ixi\n",
              "1794     sub-IXI653  46.220397    1  ixi\n",
              "1795     sub-IXI662  41.741273    1  ixi\n",
              "\n",
              "[1796 rows x 4 columns]"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cov = pd.merge(hcp, ixi, on=[\"participant_id\", \"age\", \"sex\", \"site\"], how='outer')\n",
        "cov"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 120,
      "metadata": {
        "id": "gkaf63EfPxQe"
      },
      "outputs": [],
      "source": [
        "sns.set(font_scale=1.5, style='darkgrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 121,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "ao2iVSIVPxQe",
        "outputId": "40637c54-703b-4639-fe57-e29a3d6205ab"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<seaborn.axisgrid.FacetGrid at 0x7f2a9d74b6d0>"
            ]
          },
          "execution_count": 121,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 521.4x432 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "sns.displot(cov, x=\"age\", hue=\"site\", multiple=\"stack\", height=6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 122,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "shVeUZAZPxQe",
        "outputId": "62e8e389-78ee-4198-d66d-884772e7c108"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th colspan=\"8\" halign=\"left\">age</th>\n",
              "      <th colspan=\"8\" halign=\"left\">sex</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "      <th>count</th>\n",
              "      <th>mean</th>\n",
              "      <th>std</th>\n",
              "      <th>min</th>\n",
              "      <th>25%</th>\n",
              "      <th>50%</th>\n",
              "      <th>75%</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hcp</th>\n",
              "      <td>1206.0</td>\n",
              "      <td>28.837479</td>\n",
              "      <td>3.690534</td>\n",
              "      <td>22.000000</td>\n",
              "      <td>26.000000</td>\n",
              "      <td>29.00000</td>\n",
              "      <td>32.000000</td>\n",
              "      <td>37.00000</td>\n",
              "      <td>1206.0</td>\n",
              "      <td>1.543947</td>\n",
              "      <td>0.498272</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ixi</th>\n",
              "      <td>590.0</td>\n",
              "      <td>49.476531</td>\n",
              "      <td>16.720864</td>\n",
              "      <td>19.980835</td>\n",
              "      <td>34.027721</td>\n",
              "      <td>50.61191</td>\n",
              "      <td>63.413415</td>\n",
              "      <td>86.31896</td>\n",
              "      <td>590.0</td>\n",
              "      <td>1.555932</td>\n",
              "      <td>0.497283</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         age                                                                                sex                                             \n",
              "       count       mean        std        min        25%       50%        75%       max   count      mean       std  min  25%  50%  75%  max\n",
              "site                                                                                                                                        \n",
              "hcp   1206.0  28.837479   3.690534  22.000000  26.000000  29.00000  32.000000  37.00000  1206.0  1.543947  0.498272  1.0  1.0  2.0  2.0  2.0\n",
              "ixi    590.0  49.476531  16.720864  19.980835  34.027721  50.61191  63.413415  86.31896   590.0  1.555932  0.497283  1.0  1.0  2.0  2.0  2.0"
            ]
          },
          "execution_count": 122,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "cov.groupby(['site']).describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NPUxTt-zPxQf"
      },
      "source": [
        "## Preprare brain data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QPl50oOqPxQf"
      },
      "source": [
        "### Step 3.\n",
        "\n",
        "Next, format and combine the MRI data using the following commands. The example data contains cortical thickness maps estimated by running recon-all from Freesurfer (version 6.0). The dimensionality of the data was reduced by using ROIs from the Desikan-Killiany atlas. Including the Euler number as a covariate is also recommended, as this is a proxy metric for data quality. The [Euler number](https://mathworld.wolfram.com/EulerCharacteristic.html) from each subject's recon-all output folder was extracted into a text file and is merged into the cortical thickness data frame. The Euler number is site-specific, thus, to use the same exclusion threshold across sites it is important to center the site by subtracting the site median from all subjects at a site. Then take the square root and multiply by negative one and exclude any subjects with a square root above 10."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lTaweyNcPxQf"
      },
      "source": [
        "Here is some pseudo-code (run from a terminal in the folder that has all subject's recon-all output folders) that was used to extract these ROIs:\n",
        "\n",
        "```export SUBJECTS_DIR=/path/to/study/freesurfer_data/```\n",
        "\n",
        "```aparcstats2table --subject sub-* --hemi lh --meas thickness --tablefile HCP1200_aparc_lh_thickness.txt```\n",
        "\n",
        "```aparcstats2table --subject sub-* --hemi rh --meas thickness --tablefile HCP1200_aparc_rh_thickness.txt```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 123,
      "metadata": {
        "id": "IRWlg7lMPxQg"
      },
      "outputs": [],
      "source": [
        "hcpya = pd.read_csv('/home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/data/HCP1200_aparc_thickness.csv')\n",
        "ixi = pd.read_csv('/home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/data/IXI_aparc_thickness.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 124,
      "metadata": {
        "id": "87MOUvN3PxQg"
      },
      "outputs": [],
      "source": [
        "brain_all = pd.merge(ixi, hcpya, how='outer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8w7w9RYWPxQg"
      },
      "source": [
        "We extracted the euler number from each subject's recon-all output folder into a text file and we now need to format and combine these into our brain dataframe. \n",
        "\n",
        "Below is psuedo code for how we extracted the euler number from the recon-all.log for each subject. Run this from the terminal in the folder where your subjects recon-all output folders are located. This assumes that all of your subject IDs start with \"sub-\" prefix.\n",
        "\n",
        "```for i in sub-*; do if [[ -e ${i}/scripts/recon-all.log ]]; then cat ${i}/scripts/recon-all.log | grep -A 1 \"Computing euler\" > temp_log; lh_en=`cat temp_log | head -2 | tail -1 | awk -F '=' '{print $2}' | awk -F ',' '{print $1}'`; rh_en=`cat temp_log | head -2 | tail -1 | awk -F '=' '{print $3}'`; echo \"${i}, ${lh_en}, ${rh_en}\" >> euler.csv; echo ${i}; fi; done```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 125,
      "metadata": {
        "id": "tGvdOT0PPxQg"
      },
      "outputs": [],
      "source": [
        "hcp_euler = pd.read_csv('/home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/data/hcp-ya_euler.csv')\n",
        "ixi_euler = pd.read_csv('/home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/data/ixi_euler.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 126,
      "metadata": {
        "id": "DYyZpHQKPxQg"
      },
      "outputs": [],
      "source": [
        "hcp_euler['site'] = 'hcp'\n",
        "ixi_euler['site'] = 'ixi'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 127,
      "metadata": {
        "id": "6JPbNhc0PxQh"
      },
      "outputs": [],
      "source": [
        "hcp_euler.replace(r'^\\s*$', np.nan, regex=True, inplace=True)\n",
        "ixi_euler.replace(r'^\\s*$', np.nan, regex=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 128,
      "metadata": {
        "id": "hiQZiCgVPxQh"
      },
      "outputs": [],
      "source": [
        "hcp_euler.dropna(inplace=True)\n",
        "ixi_euler.dropna(inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 129,
      "metadata": {
        "id": "iBPeQrBbPxQh"
      },
      "outputs": [],
      "source": [
        "hcp_euler['rh_euler'] = hcp_euler['rh_euler'].astype(int)\n",
        "hcp_euler['lh_euler'] = hcp_euler['lh_euler'].astype(int)\n",
        "ixi_euler['rh_euler'] = ixi_euler['rh_euler'].astype(int)\n",
        "ixi_euler['lh_euler'] = ixi_euler['lh_euler'].astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 130,
      "metadata": {
        "id": "J6P9iTq0PxQh"
      },
      "outputs": [],
      "source": [
        "df_euler = pd.merge(hcp_euler, ixi_euler, on=['participant_id', 'lh_euler', 'rh_euler', 'site'], how='outer')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S23mcO6tPxQh"
      },
      "source": [
        "Finally, we need to center the euler number for each site. The euler number is very site-specific so in order to use the same exclusion threshold across sites we need to center the site by subtracting the site median from all subjects at a site. Then we will take the square root and multiply by negative one and exclude any subjects with a square root above 10. This choice of threshold is fairly random. If possible all of your data should be visually inspected to verify that the data inclusion is not too strict or too lenient. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 131,
      "metadata": {
        "id": "-OgUwExEPxQh"
      },
      "outputs": [],
      "source": [
        "df_euler['avg_euler'] = df_euler[['lh_euler','rh_euler']].mean(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 132,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "id": "ptnUX_-8PxQh",
        "outputId": "3b34f476-f82a-46f1-9397-e09d40ffee82"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lh_euler</th>\n",
              "      <th>rh_euler</th>\n",
              "      <th>avg_euler</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>site</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>hcp</th>\n",
              "      <td>-44.0</td>\n",
              "      <td>-44.0</td>\n",
              "      <td>-43.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>ixi</th>\n",
              "      <td>-58.0</td>\n",
              "      <td>-54.0</td>\n",
              "      <td>-56.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      lh_euler  rh_euler  avg_euler\n",
              "site                               \n",
              "hcp      -44.0     -44.0      -43.0\n",
              "ixi      -58.0     -54.0      -56.0"
            ]
          },
          "execution_count": 132,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_euler.groupby(by='site').median()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 133,
      "metadata": {
        "id": "qeaX-9p3PxQi"
      },
      "outputs": [],
      "source": [
        "df_euler['site_median'] = df_euler['site']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 134,
      "metadata": {
        "id": "u6DdZc0cPxQi"
      },
      "outputs": [],
      "source": [
        "df_euler['site_median'] = df_euler['site_median'].replace({'hcp':-43,'ixi':-56})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 135,
      "metadata": {
        "id": "1rfnqsOcPxQi"
      },
      "outputs": [],
      "source": [
        "df_euler['avg_euler_centered'] = df_euler['avg_euler'] - df_euler['site_median']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 136,
      "metadata": {
        "id": "dAaD2YxxPxQi"
      },
      "outputs": [],
      "source": [
        "df_euler['avg_euler_centered_neg'] = df_euler['avg_euler_centered']*-1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 137,
      "metadata": {
        "id": "eNp1aHaSPxQi"
      },
      "outputs": [],
      "source": [
        "df_euler['avg_euler_centered_neg_sqrt'] = np.sqrt(np.absolute(df_euler['avg_euler_centered_neg']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 138,
      "metadata": {
        "id": "Nxu13Y1yPxQi"
      },
      "outputs": [],
      "source": [
        "brain = pd.merge(df_euler, brain_all, on=['participant_id'], how='inner')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 139,
      "metadata": {
        "id": "DXdpbZe2PxQi"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant_id</th>\n",
              "      <th>lh_euler</th>\n",
              "      <th>rh_euler</th>\n",
              "      <th>site</th>\n",
              "      <th>avg_euler</th>\n",
              "      <th>site_median</th>\n",
              "      <th>avg_euler_centered</th>\n",
              "      <th>avg_euler_centered_neg</th>\n",
              "      <th>avg_euler_centered_neg_sqrt</th>\n",
              "      <th>lh_bankssts_thickness</th>\n",
              "      <th>lh_caudalanteriorcingulate_thickness</th>\n",
              "      <th>lh_caudalmiddlefrontal_thickness</th>\n",
              "      <th>lh_cuneus_thickness</th>\n",
              "      <th>lh_entorhinal_thickness</th>\n",
              "      <th>lh_fusiform_thickness</th>\n",
              "      <th>lh_inferiorparietal_thickness</th>\n",
              "      <th>lh_inferiortemporal_thickness</th>\n",
              "      <th>lh_isthmuscingulate_thickness</th>\n",
              "      <th>lh_lateraloccipital_thickness</th>\n",
              "      <th>lh_lateralorbitofrontal_thickness</th>\n",
              "      <th>lh_lingual_thickness</th>\n",
              "      <th>lh_medialorbitofrontal_thickness</th>\n",
              "      <th>lh_middletemporal_thickness</th>\n",
              "      <th>lh_parahippocampal_thickness</th>\n",
              "      <th>lh_paracentral_thickness</th>\n",
              "      <th>lh_parsopercularis_thickness</th>\n",
              "      <th>lh_parsorbitalis_thickness</th>\n",
              "      <th>lh_parstriangularis_thickness</th>\n",
              "      <th>lh_pericalcarine_thickness</th>\n",
              "      <th>lh_postcentral_thickness</th>\n",
              "      <th>lh_posteriorcingulate_thickness</th>\n",
              "      <th>lh_precentral_thickness</th>\n",
              "      <th>lh_precuneus_thickness</th>\n",
              "      <th>lh_rostralanteriorcingulate_thickness</th>\n",
              "      <th>lh_rostralmiddlefrontal_thickness</th>\n",
              "      <th>lh_superiorfrontal_thickness</th>\n",
              "      <th>lh_superiorparietal_thickness</th>\n",
              "      <th>lh_superiortemporal_thickness</th>\n",
              "      <th>lh_supramarginal_thickness</th>\n",
              "      <th>lh_frontalpole_thickness</th>\n",
              "      <th>lh_temporalpole_thickness</th>\n",
              "      <th>lh_transversetemporal_thickness</th>\n",
              "      <th>lh_insula_thickness</th>\n",
              "      <th>lh_MeanThickness_thickness</th>\n",
              "      <th>BrainSegVolNotVent</th>\n",
              "      <th>eTIV</th>\n",
              "      <th>rh_bankssts_thickness</th>\n",
              "      <th>rh_caudalanteriorcingulate_thickness</th>\n",
              "      <th>rh_caudalmiddlefrontal_thickness</th>\n",
              "      <th>rh_cuneus_thickness</th>\n",
              "      <th>rh_entorhinal_thickness</th>\n",
              "      <th>rh_fusiform_thickness</th>\n",
              "      <th>rh_inferiorparietal_thickness</th>\n",
              "      <th>rh_inferiortemporal_thickness</th>\n",
              "      <th>rh_isthmuscingulate_thickness</th>\n",
              "      <th>rh_lateraloccipital_thickness</th>\n",
              "      <th>rh_lateralorbitofrontal_thickness</th>\n",
              "      <th>rh_lingual_thickness</th>\n",
              "      <th>rh_medialorbitofrontal_thickness</th>\n",
              "      <th>rh_middletemporal_thickness</th>\n",
              "      <th>rh_parahippocampal_thickness</th>\n",
              "      <th>rh_paracentral_thickness</th>\n",
              "      <th>rh_parsopercularis_thickness</th>\n",
              "      <th>rh_parsorbitalis_thickness</th>\n",
              "      <th>rh_parstriangularis_thickness</th>\n",
              "      <th>rh_pericalcarine_thickness</th>\n",
              "      <th>rh_postcentral_thickness</th>\n",
              "      <th>rh_posteriorcingulate_thickness</th>\n",
              "      <th>rh_precentral_thickness</th>\n",
              "      <th>rh_precuneus_thickness</th>\n",
              "      <th>rh_rostralanteriorcingulate_thickness</th>\n",
              "      <th>rh_rostralmiddlefrontal_thickness</th>\n",
              "      <th>rh_superiorfrontal_thickness</th>\n",
              "      <th>rh_superiorparietal_thickness</th>\n",
              "      <th>rh_superiortemporal_thickness</th>\n",
              "      <th>rh_supramarginal_thickness</th>\n",
              "      <th>rh_frontalpole_thickness</th>\n",
              "      <th>rh_temporalpole_thickness</th>\n",
              "      <th>rh_transversetemporal_thickness</th>\n",
              "      <th>rh_insula_thickness</th>\n",
              "      <th>rh_MeanThickness_thickness</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sub-100206</td>\n",
              "      <td>-62</td>\n",
              "      <td>-50</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-56.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.605551</td>\n",
              "      <td>2.733</td>\n",
              "      <td>2.682</td>\n",
              "      <td>2.765</td>\n",
              "      <td>2.185</td>\n",
              "      <td>3.388</td>\n",
              "      <td>2.957</td>\n",
              "      <td>2.583</td>\n",
              "      <td>3.065</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.304</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.573</td>\n",
              "      <td>3.011</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.822</td>\n",
              "      <td>2.719</td>\n",
              "      <td>2.562</td>\n",
              "      <td>2.199</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.610</td>\n",
              "      <td>2.661</td>\n",
              "      <td>2.479</td>\n",
              "      <td>3.102</td>\n",
              "      <td>2.580</td>\n",
              "      <td>2.827</td>\n",
              "      <td>2.235</td>\n",
              "      <td>3.017</td>\n",
              "      <td>2.701</td>\n",
              "      <td>2.780</td>\n",
              "      <td>3.237</td>\n",
              "      <td>2.677</td>\n",
              "      <td>3.198</td>\n",
              "      <td>2.63693</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.744</td>\n",
              "      <td>2.505</td>\n",
              "      <td>2.713</td>\n",
              "      <td>2.091</td>\n",
              "      <td>3.350</td>\n",
              "      <td>2.683</td>\n",
              "      <td>2.592</td>\n",
              "      <td>2.960</td>\n",
              "      <td>2.313</td>\n",
              "      <td>2.299</td>\n",
              "      <td>2.737</td>\n",
              "      <td>2.094</td>\n",
              "      <td>2.620</td>\n",
              "      <td>2.987</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.377</td>\n",
              "      <td>2.823</td>\n",
              "      <td>2.967</td>\n",
              "      <td>2.624</td>\n",
              "      <td>2.160</td>\n",
              "      <td>2.215</td>\n",
              "      <td>2.505</td>\n",
              "      <td>2.632</td>\n",
              "      <td>2.444</td>\n",
              "      <td>3.115</td>\n",
              "      <td>2.515</td>\n",
              "      <td>2.731</td>\n",
              "      <td>2.278</td>\n",
              "      <td>3.032</td>\n",
              "      <td>2.660</td>\n",
              "      <td>2.641</td>\n",
              "      <td>3.579</td>\n",
              "      <td>3.147</td>\n",
              "      <td>3.278</td>\n",
              "      <td>2.58316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sub-100307</td>\n",
              "      <td>-38</td>\n",
              "      <td>-90</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-64.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>-21.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>4.582576</td>\n",
              "      <td>2.713</td>\n",
              "      <td>3.253</td>\n",
              "      <td>2.795</td>\n",
              "      <td>1.957</td>\n",
              "      <td>3.389</td>\n",
              "      <td>3.062</td>\n",
              "      <td>2.637</td>\n",
              "      <td>2.968</td>\n",
              "      <td>2.533</td>\n",
              "      <td>2.259</td>\n",
              "      <td>3.155</td>\n",
              "      <td>2.145</td>\n",
              "      <td>2.730</td>\n",
              "      <td>3.075</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.572</td>\n",
              "      <td>2.917</td>\n",
              "      <td>3.165</td>\n",
              "      <td>2.707</td>\n",
              "      <td>1.891</td>\n",
              "      <td>2.225</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.808</td>\n",
              "      <td>2.513</td>\n",
              "      <td>3.057</td>\n",
              "      <td>2.791</td>\n",
              "      <td>2.948</td>\n",
              "      <td>2.247</td>\n",
              "      <td>2.986</td>\n",
              "      <td>2.684</td>\n",
              "      <td>3.649</td>\n",
              "      <td>3.612</td>\n",
              "      <td>2.700</td>\n",
              "      <td>3.349</td>\n",
              "      <td>2.70653</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.879</td>\n",
              "      <td>3.020</td>\n",
              "      <td>2.802</td>\n",
              "      <td>2.117</td>\n",
              "      <td>3.685</td>\n",
              "      <td>2.995</td>\n",
              "      <td>2.625</td>\n",
              "      <td>3.076</td>\n",
              "      <td>2.288</td>\n",
              "      <td>2.403</td>\n",
              "      <td>3.028</td>\n",
              "      <td>2.179</td>\n",
              "      <td>3.123</td>\n",
              "      <td>3.187</td>\n",
              "      <td>2.814</td>\n",
              "      <td>2.587</td>\n",
              "      <td>3.080</td>\n",
              "      <td>2.978</td>\n",
              "      <td>2.793</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.166</td>\n",
              "      <td>2.743</td>\n",
              "      <td>2.758</td>\n",
              "      <td>2.601</td>\n",
              "      <td>3.012</td>\n",
              "      <td>2.789</td>\n",
              "      <td>3.087</td>\n",
              "      <td>2.250</td>\n",
              "      <td>3.151</td>\n",
              "      <td>2.766</td>\n",
              "      <td>3.675</td>\n",
              "      <td>4.026</td>\n",
              "      <td>2.819</td>\n",
              "      <td>3.002</td>\n",
              "      <td>2.74140</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sub-100408</td>\n",
              "      <td>-70</td>\n",
              "      <td>-42</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-56.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.605551</td>\n",
              "      <td>2.532</td>\n",
              "      <td>2.588</td>\n",
              "      <td>2.612</td>\n",
              "      <td>2.127</td>\n",
              "      <td>3.310</td>\n",
              "      <td>2.870</td>\n",
              "      <td>2.515</td>\n",
              "      <td>2.840</td>\n",
              "      <td>2.257</td>\n",
              "      <td>2.247</td>\n",
              "      <td>2.715</td>\n",
              "      <td>2.294</td>\n",
              "      <td>2.530</td>\n",
              "      <td>2.868</td>\n",
              "      <td>2.210</td>\n",
              "      <td>2.489</td>\n",
              "      <td>2.579</td>\n",
              "      <td>2.790</td>\n",
              "      <td>2.502</td>\n",
              "      <td>1.919</td>\n",
              "      <td>2.191</td>\n",
              "      <td>2.575</td>\n",
              "      <td>2.610</td>\n",
              "      <td>2.498</td>\n",
              "      <td>2.833</td>\n",
              "      <td>2.525</td>\n",
              "      <td>2.730</td>\n",
              "      <td>2.311</td>\n",
              "      <td>2.887</td>\n",
              "      <td>2.567</td>\n",
              "      <td>2.748</td>\n",
              "      <td>3.344</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.968</td>\n",
              "      <td>2.56161</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.232</td>\n",
              "      <td>2.552</td>\n",
              "      <td>2.173</td>\n",
              "      <td>3.232</td>\n",
              "      <td>2.816</td>\n",
              "      <td>2.582</td>\n",
              "      <td>2.857</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.272</td>\n",
              "      <td>2.905</td>\n",
              "      <td>2.223</td>\n",
              "      <td>2.762</td>\n",
              "      <td>2.991</td>\n",
              "      <td>2.423</td>\n",
              "      <td>2.536</td>\n",
              "      <td>2.820</td>\n",
              "      <td>2.742</td>\n",
              "      <td>2.524</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.214</td>\n",
              "      <td>2.481</td>\n",
              "      <td>2.646</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.862</td>\n",
              "      <td>2.502</td>\n",
              "      <td>2.760</td>\n",
              "      <td>2.282</td>\n",
              "      <td>2.846</td>\n",
              "      <td>2.554</td>\n",
              "      <td>2.840</td>\n",
              "      <td>3.102</td>\n",
              "      <td>2.532</td>\n",
              "      <td>2.947</td>\n",
              "      <td>2.57804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sub-100610</td>\n",
              "      <td>-30</td>\n",
              "      <td>-38</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-34.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-9.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.683</td>\n",
              "      <td>3.029</td>\n",
              "      <td>2.735</td>\n",
              "      <td>2.223</td>\n",
              "      <td>3.664</td>\n",
              "      <td>2.873</td>\n",
              "      <td>2.547</td>\n",
              "      <td>3.062</td>\n",
              "      <td>2.471</td>\n",
              "      <td>2.238</td>\n",
              "      <td>2.881</td>\n",
              "      <td>2.149</td>\n",
              "      <td>2.412</td>\n",
              "      <td>3.014</td>\n",
              "      <td>3.324</td>\n",
              "      <td>2.518</td>\n",
              "      <td>2.849</td>\n",
              "      <td>2.865</td>\n",
              "      <td>2.886</td>\n",
              "      <td>2.009</td>\n",
              "      <td>2.233</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.512</td>\n",
              "      <td>3.153</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.947</td>\n",
              "      <td>2.272</td>\n",
              "      <td>2.870</td>\n",
              "      <td>2.671</td>\n",
              "      <td>3.020</td>\n",
              "      <td>3.427</td>\n",
              "      <td>2.591</td>\n",
              "      <td>3.156</td>\n",
              "      <td>2.65715</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.743</td>\n",
              "      <td>2.623</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.120</td>\n",
              "      <td>3.737</td>\n",
              "      <td>2.912</td>\n",
              "      <td>2.578</td>\n",
              "      <td>2.800</td>\n",
              "      <td>2.381</td>\n",
              "      <td>2.300</td>\n",
              "      <td>2.872</td>\n",
              "      <td>2.304</td>\n",
              "      <td>2.900</td>\n",
              "      <td>3.013</td>\n",
              "      <td>2.775</td>\n",
              "      <td>2.536</td>\n",
              "      <td>2.779</td>\n",
              "      <td>2.842</td>\n",
              "      <td>2.851</td>\n",
              "      <td>2.142</td>\n",
              "      <td>2.154</td>\n",
              "      <td>2.528</td>\n",
              "      <td>2.750</td>\n",
              "      <td>2.512</td>\n",
              "      <td>3.336</td>\n",
              "      <td>2.620</td>\n",
              "      <td>3.013</td>\n",
              "      <td>2.257</td>\n",
              "      <td>2.901</td>\n",
              "      <td>2.632</td>\n",
              "      <td>2.994</td>\n",
              "      <td>3.723</td>\n",
              "      <td>2.721</td>\n",
              "      <td>3.068</td>\n",
              "      <td>2.64460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sub-101006</td>\n",
              "      <td>-16</td>\n",
              "      <td>-24</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>23.0</td>\n",
              "      <td>-23.0</td>\n",
              "      <td>4.795832</td>\n",
              "      <td>2.632</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.839</td>\n",
              "      <td>1.977</td>\n",
              "      <td>3.236</td>\n",
              "      <td>2.841</td>\n",
              "      <td>2.522</td>\n",
              "      <td>2.846</td>\n",
              "      <td>2.300</td>\n",
              "      <td>2.231</td>\n",
              "      <td>2.658</td>\n",
              "      <td>1.946</td>\n",
              "      <td>2.637</td>\n",
              "      <td>2.910</td>\n",
              "      <td>2.616</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.899</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.587</td>\n",
              "      <td>2.043</td>\n",
              "      <td>2.099</td>\n",
              "      <td>2.441</td>\n",
              "      <td>2.861</td>\n",
              "      <td>2.341</td>\n",
              "      <td>3.004</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.204</td>\n",
              "      <td>2.858</td>\n",
              "      <td>2.535</td>\n",
              "      <td>2.942</td>\n",
              "      <td>3.211</td>\n",
              "      <td>2.791</td>\n",
              "      <td>2.995</td>\n",
              "      <td>2.55864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.742</td>\n",
              "      <td>1.890</td>\n",
              "      <td>2.866</td>\n",
              "      <td>2.065</td>\n",
              "      <td>3.234</td>\n",
              "      <td>2.867</td>\n",
              "      <td>2.516</td>\n",
              "      <td>2.833</td>\n",
              "      <td>1.951</td>\n",
              "      <td>2.229</td>\n",
              "      <td>2.878</td>\n",
              "      <td>2.007</td>\n",
              "      <td>2.626</td>\n",
              "      <td>2.981</td>\n",
              "      <td>2.836</td>\n",
              "      <td>2.581</td>\n",
              "      <td>2.960</td>\n",
              "      <td>2.669</td>\n",
              "      <td>2.639</td>\n",
              "      <td>1.993</td>\n",
              "      <td>2.156</td>\n",
              "      <td>2.008</td>\n",
              "      <td>2.771</td>\n",
              "      <td>2.543</td>\n",
              "      <td>2.807</td>\n",
              "      <td>2.581</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.313</td>\n",
              "      <td>2.879</td>\n",
              "      <td>2.674</td>\n",
              "      <td>2.658</td>\n",
              "      <td>3.540</td>\n",
              "      <td>2.726</td>\n",
              "      <td>2.958</td>\n",
              "      <td>2.57177</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1684</th>\n",
              "      <td>sub-IXI651</td>\n",
              "      <td>-34</td>\n",
              "      <td>-30</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-32.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>24.0</td>\n",
              "      <td>-24.0</td>\n",
              "      <td>4.898979</td>\n",
              "      <td>2.529</td>\n",
              "      <td>3.073</td>\n",
              "      <td>2.675</td>\n",
              "      <td>2.124</td>\n",
              "      <td>4.006</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.675</td>\n",
              "      <td>2.969</td>\n",
              "      <td>2.413</td>\n",
              "      <td>2.211</td>\n",
              "      <td>2.930</td>\n",
              "      <td>2.046</td>\n",
              "      <td>2.634</td>\n",
              "      <td>3.128</td>\n",
              "      <td>3.233</td>\n",
              "      <td>2.382</td>\n",
              "      <td>2.546</td>\n",
              "      <td>2.640</td>\n",
              "      <td>2.606</td>\n",
              "      <td>1.688</td>\n",
              "      <td>1.934</td>\n",
              "      <td>2.765</td>\n",
              "      <td>2.541</td>\n",
              "      <td>2.446</td>\n",
              "      <td>2.918</td>\n",
              "      <td>2.560</td>\n",
              "      <td>2.810</td>\n",
              "      <td>2.160</td>\n",
              "      <td>2.881</td>\n",
              "      <td>2.479</td>\n",
              "      <td>2.535</td>\n",
              "      <td>3.859</td>\n",
              "      <td>2.372</td>\n",
              "      <td>3.251</td>\n",
              "      <td>2.57618</td>\n",
              "      <td>1036518.0</td>\n",
              "      <td>1.357712e+06</td>\n",
              "      <td>2.539</td>\n",
              "      <td>3.135</td>\n",
              "      <td>2.692</td>\n",
              "      <td>1.766</td>\n",
              "      <td>3.721</td>\n",
              "      <td>2.868</td>\n",
              "      <td>2.631</td>\n",
              "      <td>2.944</td>\n",
              "      <td>2.433</td>\n",
              "      <td>2.333</td>\n",
              "      <td>2.635</td>\n",
              "      <td>2.127</td>\n",
              "      <td>2.539</td>\n",
              "      <td>3.063</td>\n",
              "      <td>3.312</td>\n",
              "      <td>2.167</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.876</td>\n",
              "      <td>2.657</td>\n",
              "      <td>1.662</td>\n",
              "      <td>1.916</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.267</td>\n",
              "      <td>2.280</td>\n",
              "      <td>3.515</td>\n",
              "      <td>2.535</td>\n",
              "      <td>2.694</td>\n",
              "      <td>2.073</td>\n",
              "      <td>2.935</td>\n",
              "      <td>2.546</td>\n",
              "      <td>2.520</td>\n",
              "      <td>3.997</td>\n",
              "      <td>2.142</td>\n",
              "      <td>3.167</td>\n",
              "      <td>2.51866</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1685</th>\n",
              "      <td>sub-IXI652</td>\n",
              "      <td>-38</td>\n",
              "      <td>-54</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-46.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>3.162278</td>\n",
              "      <td>2.499</td>\n",
              "      <td>2.725</td>\n",
              "      <td>2.587</td>\n",
              "      <td>1.670</td>\n",
              "      <td>3.921</td>\n",
              "      <td>2.891</td>\n",
              "      <td>2.444</td>\n",
              "      <td>2.995</td>\n",
              "      <td>2.641</td>\n",
              "      <td>2.172</td>\n",
              "      <td>2.698</td>\n",
              "      <td>1.869</td>\n",
              "      <td>2.560</td>\n",
              "      <td>2.985</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.138</td>\n",
              "      <td>2.608</td>\n",
              "      <td>2.699</td>\n",
              "      <td>2.528</td>\n",
              "      <td>1.487</td>\n",
              "      <td>2.027</td>\n",
              "      <td>2.478</td>\n",
              "      <td>2.562</td>\n",
              "      <td>2.454</td>\n",
              "      <td>3.044</td>\n",
              "      <td>2.426</td>\n",
              "      <td>2.722</td>\n",
              "      <td>2.115</td>\n",
              "      <td>2.822</td>\n",
              "      <td>2.587</td>\n",
              "      <td>2.721</td>\n",
              "      <td>4.085</td>\n",
              "      <td>2.410</td>\n",
              "      <td>3.116</td>\n",
              "      <td>2.51061</td>\n",
              "      <td>1305959.0</td>\n",
              "      <td>1.715795e+06</td>\n",
              "      <td>2.549</td>\n",
              "      <td>2.890</td>\n",
              "      <td>2.675</td>\n",
              "      <td>1.663</td>\n",
              "      <td>3.511</td>\n",
              "      <td>2.841</td>\n",
              "      <td>2.486</td>\n",
              "      <td>2.934</td>\n",
              "      <td>2.723</td>\n",
              "      <td>2.255</td>\n",
              "      <td>2.797</td>\n",
              "      <td>1.876</td>\n",
              "      <td>2.679</td>\n",
              "      <td>2.845</td>\n",
              "      <td>2.702</td>\n",
              "      <td>2.273</td>\n",
              "      <td>2.803</td>\n",
              "      <td>2.800</td>\n",
              "      <td>2.500</td>\n",
              "      <td>1.500</td>\n",
              "      <td>2.012</td>\n",
              "      <td>2.429</td>\n",
              "      <td>2.565</td>\n",
              "      <td>2.325</td>\n",
              "      <td>2.826</td>\n",
              "      <td>2.480</td>\n",
              "      <td>2.802</td>\n",
              "      <td>1.969</td>\n",
              "      <td>2.997</td>\n",
              "      <td>2.683</td>\n",
              "      <td>2.654</td>\n",
              "      <td>4.229</td>\n",
              "      <td>2.416</td>\n",
              "      <td>3.353</td>\n",
              "      <td>2.51731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1686</th>\n",
              "      <td>sub-IXI653</td>\n",
              "      <td>-58</td>\n",
              "      <td>-54</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-56.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.825</td>\n",
              "      <td>2.756</td>\n",
              "      <td>2.518</td>\n",
              "      <td>1.858</td>\n",
              "      <td>3.421</td>\n",
              "      <td>3.018</td>\n",
              "      <td>2.758</td>\n",
              "      <td>3.153</td>\n",
              "      <td>2.273</td>\n",
              "      <td>2.420</td>\n",
              "      <td>2.944</td>\n",
              "      <td>2.004</td>\n",
              "      <td>2.697</td>\n",
              "      <td>3.000</td>\n",
              "      <td>3.279</td>\n",
              "      <td>2.127</td>\n",
              "      <td>2.635</td>\n",
              "      <td>3.073</td>\n",
              "      <td>2.508</td>\n",
              "      <td>1.437</td>\n",
              "      <td>1.979</td>\n",
              "      <td>2.440</td>\n",
              "      <td>2.238</td>\n",
              "      <td>2.416</td>\n",
              "      <td>2.899</td>\n",
              "      <td>2.577</td>\n",
              "      <td>2.682</td>\n",
              "      <td>2.053</td>\n",
              "      <td>3.014</td>\n",
              "      <td>2.580</td>\n",
              "      <td>2.962</td>\n",
              "      <td>4.065</td>\n",
              "      <td>2.387</td>\n",
              "      <td>3.234</td>\n",
              "      <td>2.55112</td>\n",
              "      <td>1272881.0</td>\n",
              "      <td>1.663991e+06</td>\n",
              "      <td>2.802</td>\n",
              "      <td>2.378</td>\n",
              "      <td>2.369</td>\n",
              "      <td>1.885</td>\n",
              "      <td>3.901</td>\n",
              "      <td>2.844</td>\n",
              "      <td>2.782</td>\n",
              "      <td>3.024</td>\n",
              "      <td>2.468</td>\n",
              "      <td>2.414</td>\n",
              "      <td>2.976</td>\n",
              "      <td>2.090</td>\n",
              "      <td>2.745</td>\n",
              "      <td>3.011</td>\n",
              "      <td>3.187</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.646</td>\n",
              "      <td>2.804</td>\n",
              "      <td>2.527</td>\n",
              "      <td>1.668</td>\n",
              "      <td>1.826</td>\n",
              "      <td>2.528</td>\n",
              "      <td>2.250</td>\n",
              "      <td>2.390</td>\n",
              "      <td>3.174</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.669</td>\n",
              "      <td>2.049</td>\n",
              "      <td>2.784</td>\n",
              "      <td>2.637</td>\n",
              "      <td>2.933</td>\n",
              "      <td>3.881</td>\n",
              "      <td>2.313</td>\n",
              "      <td>3.348</td>\n",
              "      <td>2.51458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687</th>\n",
              "      <td>sub-IXI661</td>\n",
              "      <td>-54</td>\n",
              "      <td>-64</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-59.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>-3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.732051</td>\n",
              "      <td>2.913</td>\n",
              "      <td>2.698</td>\n",
              "      <td>2.608</td>\n",
              "      <td>1.743</td>\n",
              "      <td>3.898</td>\n",
              "      <td>3.100</td>\n",
              "      <td>2.638</td>\n",
              "      <td>3.107</td>\n",
              "      <td>2.252</td>\n",
              "      <td>2.364</td>\n",
              "      <td>2.997</td>\n",
              "      <td>1.938</td>\n",
              "      <td>2.674</td>\n",
              "      <td>3.281</td>\n",
              "      <td>3.135</td>\n",
              "      <td>2.207</td>\n",
              "      <td>2.596</td>\n",
              "      <td>3.286</td>\n",
              "      <td>2.648</td>\n",
              "      <td>1.582</td>\n",
              "      <td>2.081</td>\n",
              "      <td>2.483</td>\n",
              "      <td>2.503</td>\n",
              "      <td>2.423</td>\n",
              "      <td>3.391</td>\n",
              "      <td>2.610</td>\n",
              "      <td>2.824</td>\n",
              "      <td>2.121</td>\n",
              "      <td>3.017</td>\n",
              "      <td>2.859</td>\n",
              "      <td>2.931</td>\n",
              "      <td>3.996</td>\n",
              "      <td>2.380</td>\n",
              "      <td>3.288</td>\n",
              "      <td>2.62388</td>\n",
              "      <td>946331.0</td>\n",
              "      <td>1.363561e+06</td>\n",
              "      <td>3.072</td>\n",
              "      <td>2.805</td>\n",
              "      <td>2.539</td>\n",
              "      <td>1.978</td>\n",
              "      <td>3.730</td>\n",
              "      <td>3.129</td>\n",
              "      <td>2.639</td>\n",
              "      <td>3.108</td>\n",
              "      <td>2.382</td>\n",
              "      <td>2.505</td>\n",
              "      <td>2.963</td>\n",
              "      <td>1.795</td>\n",
              "      <td>2.673</td>\n",
              "      <td>2.999</td>\n",
              "      <td>2.543</td>\n",
              "      <td>2.194</td>\n",
              "      <td>2.713</td>\n",
              "      <td>3.136</td>\n",
              "      <td>2.704</td>\n",
              "      <td>1.483</td>\n",
              "      <td>1.900</td>\n",
              "      <td>2.709</td>\n",
              "      <td>2.499</td>\n",
              "      <td>2.461</td>\n",
              "      <td>3.190</td>\n",
              "      <td>2.621</td>\n",
              "      <td>2.907</td>\n",
              "      <td>2.094</td>\n",
              "      <td>2.798</td>\n",
              "      <td>2.547</td>\n",
              "      <td>3.065</td>\n",
              "      <td>4.358</td>\n",
              "      <td>2.219</td>\n",
              "      <td>3.047</td>\n",
              "      <td>2.59026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1688</th>\n",
              "      <td>sub-IXI662</td>\n",
              "      <td>-32</td>\n",
              "      <td>-26</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-29.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>27.0</td>\n",
              "      <td>-27.0</td>\n",
              "      <td>5.196152</td>\n",
              "      <td>2.478</td>\n",
              "      <td>3.011</td>\n",
              "      <td>2.534</td>\n",
              "      <td>1.823</td>\n",
              "      <td>3.579</td>\n",
              "      <td>3.030</td>\n",
              "      <td>2.365</td>\n",
              "      <td>2.949</td>\n",
              "      <td>2.205</td>\n",
              "      <td>2.229</td>\n",
              "      <td>2.926</td>\n",
              "      <td>1.980</td>\n",
              "      <td>2.659</td>\n",
              "      <td>3.009</td>\n",
              "      <td>3.102</td>\n",
              "      <td>2.145</td>\n",
              "      <td>2.548</td>\n",
              "      <td>3.204</td>\n",
              "      <td>2.682</td>\n",
              "      <td>1.562</td>\n",
              "      <td>2.113</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.531</td>\n",
              "      <td>2.232</td>\n",
              "      <td>2.992</td>\n",
              "      <td>2.549</td>\n",
              "      <td>2.600</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.921</td>\n",
              "      <td>2.488</td>\n",
              "      <td>3.119</td>\n",
              "      <td>3.667</td>\n",
              "      <td>2.170</td>\n",
              "      <td>3.415</td>\n",
              "      <td>2.53004</td>\n",
              "      <td>1301484.0</td>\n",
              "      <td>1.726568e+06</td>\n",
              "      <td>2.600</td>\n",
              "      <td>2.653</td>\n",
              "      <td>2.502</td>\n",
              "      <td>1.854</td>\n",
              "      <td>3.561</td>\n",
              "      <td>3.055</td>\n",
              "      <td>2.406</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.337</td>\n",
              "      <td>3.021</td>\n",
              "      <td>2.083</td>\n",
              "      <td>2.692</td>\n",
              "      <td>3.031</td>\n",
              "      <td>2.822</td>\n",
              "      <td>2.137</td>\n",
              "      <td>2.684</td>\n",
              "      <td>2.906</td>\n",
              "      <td>2.627</td>\n",
              "      <td>1.709</td>\n",
              "      <td>2.058</td>\n",
              "      <td>2.763</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.445</td>\n",
              "      <td>3.139</td>\n",
              "      <td>2.636</td>\n",
              "      <td>2.725</td>\n",
              "      <td>2.037</td>\n",
              "      <td>2.911</td>\n",
              "      <td>2.545</td>\n",
              "      <td>2.812</td>\n",
              "      <td>4.012</td>\n",
              "      <td>2.485</td>\n",
              "      <td>3.295</td>\n",
              "      <td>2.55809</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1685 rows × 81 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     participant_id  lh_euler  rh_euler site  avg_euler  site_median  avg_euler_centered  avg_euler_centered_neg  avg_euler_centered_neg_sqrt  lh_bankssts_thickness  lh_caudalanteriorcingulate_thickness  lh_caudalmiddlefrontal_thickness  lh_cuneus_thickness  lh_entorhinal_thickness  lh_fusiform_thickness  lh_inferiorparietal_thickness  lh_inferiortemporal_thickness  lh_isthmuscingulate_thickness  lh_lateraloccipital_thickness  lh_lateralorbitofrontal_thickness  lh_lingual_thickness  lh_medialorbitofrontal_thickness  lh_middletemporal_thickness  lh_parahippocampal_thickness  lh_paracentral_thickness  lh_parsopercularis_thickness  lh_parsorbitalis_thickness  lh_parstriangularis_thickness  lh_pericalcarine_thickness  lh_postcentral_thickness  lh_posteriorcingulate_thickness  lh_precentral_thickness  lh_precuneus_thickness  lh_rostralanteriorcingulate_thickness  lh_rostralmiddlefrontal_thickness  lh_superiorfrontal_thickness  lh_superiorparietal_thickness  lh_superiortemporal_thickness  \\\n",
              "0        sub-100206       -62       -50  hcp      -56.0          -43               -13.0                    13.0                     3.605551                  2.733                                 2.682                             2.765                2.185                    3.388                  2.957                          2.583                          3.065                          2.428                          2.304                              2.916                 2.259                             2.573                        3.011                         2.916                     2.487                         2.822                       2.719                          2.562                       2.199                     2.259                            2.610                    2.661                   2.479                                  3.102                              2.580                         2.827                          2.235                          3.017   \n",
              "1        sub-100307       -38       -90  hcp      -64.0          -43               -21.0                    21.0                     4.582576                  2.713                                 3.253                             2.795                1.957                    3.389                  3.062                          2.637                          2.968                          2.533                          2.259                              3.155                 2.145                             2.730                        3.075                         2.460                     2.572                         2.917                       3.165                          2.707                       1.891                     2.225                            2.864                    2.808                   2.513                                  3.057                              2.791                         2.948                          2.247                          2.986   \n",
              "2        sub-100408       -70       -42  hcp      -56.0          -43               -13.0                    13.0                     3.605551                  2.532                                 2.588                             2.612                2.127                    3.310                  2.870                          2.515                          2.840                          2.257                          2.247                              2.715                 2.294                             2.530                        2.868                         2.210                     2.489                         2.579                       2.790                          2.502                       1.919                     2.191                            2.575                    2.610                   2.498                                  2.833                              2.525                         2.730                          2.311                          2.887   \n",
              "3        sub-100610       -30       -38  hcp      -34.0          -43                 9.0                    -9.0                     3.000000                  2.683                                 3.029                             2.735                2.223                    3.664                  2.873                          2.547                          3.062                          2.471                          2.238                              2.881                 2.149                             2.412                        3.014                         3.324                     2.518                         2.849                       2.865                          2.886                       2.009                     2.233                            2.556                    2.864                   2.512                                  3.153                              2.687                         2.947                          2.272                          2.870   \n",
              "4        sub-101006       -16       -24  hcp      -20.0          -43                23.0                   -23.0                     4.795832                  2.632                                 2.657                             2.839                1.977                    3.236                  2.841                          2.522                          2.846                          2.300                          2.231                              2.658                 1.946                             2.637                        2.910                         2.616                     2.487                         2.899                       2.729                          2.587                       2.043                     2.099                            2.441                    2.861                   2.341                                  3.004                              2.529                         2.864                          2.204                          2.858   \n",
              "...             ...       ...       ...  ...        ...          ...                 ...                     ...                          ...                    ...                                   ...                               ...                  ...                      ...                    ...                            ...                            ...                            ...                            ...                                ...                   ...                               ...                          ...                           ...                       ...                           ...                         ...                            ...                         ...                       ...                              ...                      ...                     ...                                    ...                                ...                           ...                            ...                            ...   \n",
              "1684     sub-IXI651       -34       -30  ixi      -32.0          -56                24.0                   -24.0                     4.898979                  2.529                                 3.073                             2.675                2.124                    4.006                  2.864                          2.675                          2.969                          2.413                          2.211                              2.930                 2.046                             2.634                        3.128                         3.233                     2.382                         2.546                       2.640                          2.606                       1.688                     1.934                            2.765                    2.541                   2.446                                  2.918                              2.560                         2.810                          2.160                          2.881   \n",
              "1685     sub-IXI652       -38       -54  ixi      -46.0          -56                10.0                   -10.0                     3.162278                  2.499                                 2.725                             2.587                1.670                    3.921                  2.891                          2.444                          2.995                          2.641                          2.172                              2.698                 1.869                             2.560                        2.985                         2.570                     2.138                         2.608                       2.699                          2.528                       1.487                     2.027                            2.478                    2.562                   2.454                                  3.044                              2.426                         2.722                          2.115                          2.822   \n",
              "1686     sub-IXI653       -58       -54  ixi      -56.0          -56                 0.0                    -0.0                     0.000000                  2.825                                 2.756                             2.518                1.858                    3.421                  3.018                          2.758                          3.153                          2.273                          2.420                              2.944                 2.004                             2.697                        3.000                         3.279                     2.127                         2.635                       3.073                          2.508                       1.437                     1.979                            2.440                    2.238                   2.416                                  2.899                              2.577                         2.682                          2.053                          3.014   \n",
              "1687     sub-IXI661       -54       -64  ixi      -59.0          -56                -3.0                     3.0                     1.732051                  2.913                                 2.698                             2.608                1.743                    3.898                  3.100                          2.638                          3.107                          2.252                          2.364                              2.997                 1.938                             2.674                        3.281                         3.135                     2.207                         2.596                       3.286                          2.648                       1.582                     2.081                            2.483                    2.503                   2.423                                  3.391                              2.610                         2.824                          2.121                          3.017   \n",
              "1688     sub-IXI662       -32       -26  ixi      -29.0          -56                27.0                   -27.0                     5.196152                  2.478                                 3.011                             2.534                1.823                    3.579                  3.030                          2.365                          2.949                          2.205                          2.229                              2.926                 1.980                             2.659                        3.009                         3.102                     2.145                         2.548                       3.204                          2.682                       1.562                     2.113                            2.739                    2.531                   2.232                                  2.992                              2.549                         2.600                          2.109                          2.921   \n",
              "\n",
              "      lh_supramarginal_thickness  lh_frontalpole_thickness  lh_temporalpole_thickness  lh_transversetemporal_thickness  lh_insula_thickness  lh_MeanThickness_thickness  BrainSegVolNotVent          eTIV  rh_bankssts_thickness  rh_caudalanteriorcingulate_thickness  rh_caudalmiddlefrontal_thickness  rh_cuneus_thickness  rh_entorhinal_thickness  rh_fusiform_thickness  rh_inferiorparietal_thickness  rh_inferiortemporal_thickness  rh_isthmuscingulate_thickness  rh_lateraloccipital_thickness  rh_lateralorbitofrontal_thickness  rh_lingual_thickness  rh_medialorbitofrontal_thickness  rh_middletemporal_thickness  rh_parahippocampal_thickness  rh_paracentral_thickness  rh_parsopercularis_thickness  rh_parsorbitalis_thickness  rh_parstriangularis_thickness  rh_pericalcarine_thickness  rh_postcentral_thickness  rh_posteriorcingulate_thickness  rh_precentral_thickness  rh_precuneus_thickness  rh_rostralanteriorcingulate_thickness  rh_rostralmiddlefrontal_thickness  rh_superiorfrontal_thickness  \\\n",
              "0                          2.701                     2.780                      3.237                            2.677                3.198                     2.63693                 NaN           NaN                  2.744                                 2.505                             2.713                2.091                    3.350                  2.683                          2.592                          2.960                          2.313                          2.299                              2.737                 2.094                             2.620                        2.987                         2.487                     2.377                         2.823                       2.967                          2.624                       2.160                     2.215                            2.505                    2.632                   2.444                                  3.115                              2.515                         2.731   \n",
              "1                          2.684                     3.649                      3.612                            2.700                3.349                     2.70653                 NaN           NaN                  2.879                                 3.020                             2.802                2.117                    3.685                  2.995                          2.625                          3.076                          2.288                          2.403                              3.028                 2.179                             3.123                        3.187                         2.814                     2.587                         3.080                       2.978                          2.793                       2.050                     2.166                            2.743                    2.758                   2.601                                  3.012                              2.789                         3.087   \n",
              "2                          2.567                     2.748                      3.344                            2.556                2.968                     2.56161                 NaN           NaN                  2.729                                 2.232                             2.552                2.173                    3.232                  2.816                          2.582                          2.857                          2.203                          2.272                              2.905                 2.223                             2.762                        2.991                         2.423                     2.536                         2.820                       2.742                          2.524                       2.056                     2.214                            2.481                    2.646                   2.529                                  2.862                              2.502                         2.760   \n",
              "3                          2.671                     3.020                      3.427                            2.591                3.156                     2.65715                 NaN           NaN                  2.743                                 2.623                             2.739                2.120                    3.737                  2.912                          2.578                          2.800                          2.381                          2.300                              2.872                 2.304                             2.900                        3.013                         2.775                     2.536                         2.779                       2.842                          2.851                       2.142                     2.154                            2.528                    2.750                   2.512                                  3.336                              2.620                         3.013   \n",
              "4                          2.535                     2.942                      3.211                            2.791                2.995                     2.55864                 NaN           NaN                  2.742                                 1.890                             2.866                2.065                    3.234                  2.867                          2.516                          2.833                          1.951                          2.229                              2.878                 2.007                             2.626                        2.981                         2.836                     2.581                         2.960                       2.669                          2.639                       1.993                     2.156                            2.008                    2.771                   2.543                                  2.807                              2.581                         2.864   \n",
              "...                          ...                       ...                        ...                              ...                  ...                         ...                 ...           ...                    ...                                   ...                               ...                  ...                      ...                    ...                            ...                            ...                            ...                            ...                                ...                   ...                               ...                          ...                           ...                       ...                           ...                         ...                            ...                         ...                       ...                              ...                      ...                     ...                                    ...                                ...                           ...   \n",
              "1684                       2.479                     2.535                      3.859                            2.372                3.251                     2.57618           1036518.0  1.357712e+06                  2.539                                 3.135                             2.692                1.766                    3.721                  2.868                          2.631                          2.944                          2.433                          2.333                              2.635                 2.127                             2.539                        3.063                         3.312                     2.167                         2.687                       2.876                          2.657                       1.662                     1.916                            2.687                    2.267                   2.280                                  3.515                              2.535                         2.694   \n",
              "1685                       2.587                     2.721                      4.085                            2.410                3.116                     2.51061           1305959.0  1.715795e+06                  2.549                                 2.890                             2.675                1.663                    3.511                  2.841                          2.486                          2.934                          2.723                          2.255                              2.797                 1.876                             2.679                        2.845                         2.702                     2.273                         2.803                       2.800                          2.500                       1.500                     2.012                            2.429                    2.565                   2.325                                  2.826                              2.480                         2.802   \n",
              "1686                       2.580                     2.962                      4.065                            2.387                3.234                     2.55112           1272881.0  1.663991e+06                  2.802                                 2.378                             2.369                1.885                    3.901                  2.844                          2.782                          3.024                          2.468                          2.414                              2.976                 2.090                             2.745                        3.011                         3.187                     2.018                         2.646                       2.804                          2.527                       1.668                     1.826                            2.528                    2.250                   2.390                                  3.174                              2.460                         2.669   \n",
              "1687                       2.859                     2.931                      3.996                            2.380                3.288                     2.62388            946331.0  1.363561e+06                  3.072                                 2.805                             2.539                1.978                    3.730                  3.129                          2.639                          3.108                          2.382                          2.505                              2.963                 1.795                             2.673                        2.999                         2.543                     2.194                         2.713                       3.136                          2.704                       1.483                     1.900                            2.709                    2.499                   2.461                                  3.190                              2.621                         2.907   \n",
              "1688                       2.488                     3.119                      3.667                            2.170                3.415                     2.53004           1301484.0  1.726568e+06                  2.600                                 2.653                             2.502                1.854                    3.561                  3.055                          2.406                          2.916                          2.386                          2.337                              3.021                 2.083                             2.692                        3.031                         2.822                     2.137                         2.684                       2.906                          2.627                       1.709                     2.058                            2.763                    2.386                   2.445                                  3.139                              2.636                         2.725   \n",
              "\n",
              "      rh_superiorparietal_thickness  rh_superiortemporal_thickness  rh_supramarginal_thickness  rh_frontalpole_thickness  rh_temporalpole_thickness  rh_transversetemporal_thickness  rh_insula_thickness  rh_MeanThickness_thickness  \n",
              "0                             2.278                          3.032                       2.660                     2.641                      3.579                            3.147                3.278                     2.58316  \n",
              "1                             2.250                          3.151                       2.766                     3.675                      4.026                            2.819                3.002                     2.74140  \n",
              "2                             2.282                          2.846                       2.554                     2.840                      3.102                            2.532                2.947                     2.57804  \n",
              "3                             2.257                          2.901                       2.632                     2.994                      3.723                            2.721                3.068                     2.64460  \n",
              "4                             2.313                          2.879                       2.674                     2.658                      3.540                            2.726                2.958                     2.57177  \n",
              "...                             ...                            ...                         ...                       ...                        ...                              ...                  ...                         ...  \n",
              "1684                          2.073                          2.935                       2.546                     2.520                      3.997                            2.142                3.167                     2.51866  \n",
              "1685                          1.969                          2.997                       2.683                     2.654                      4.229                            2.416                3.353                     2.51731  \n",
              "1686                          2.049                          2.784                       2.637                     2.933                      3.881                            2.313                3.348                     2.51458  \n",
              "1687                          2.094                          2.798                       2.547                     3.065                      4.358                            2.219                3.047                     2.59026  \n",
              "1688                          2.037                          2.911                       2.545                     2.812                      4.012                            2.485                3.295                     2.55809  \n",
              "\n",
              "[1685 rows x 81 columns]"
            ]
          },
          "execution_count": 139,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "brain_good = brain.query('avg_euler_centered_neg_sqrt < 10')\n",
        "brain_good"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0cbSkqolPxQj"
      },
      "source": [
        "**CRITICAL STEP:** If possible, data should be visually inspected to verify that the data inclusion is not too strict or too lenient. Subjects above the Euler number threshold should be manually checked to verify and justify their exclusion due to poor data quality. This is just one approach for automated QC used by the developers of the PCNtoolkit. Other approaches such as the ENIGMA QC pipeline or UK Biobank’s QC pipeline are also viable options for automated QC."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w77w8FyCPxQj"
      },
      "source": [
        "## Combine covariate & cortical thickness dataframes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MAiIjVVvPxQj"
      },
      "source": [
        "### Step 4.\t\n",
        "\n",
        "The normative modeling function requires the covariate predictors and brain features to be in separate text files. However, it is important to first (inner) merge them together, using the following commands, to confirm that the same subjects are in each file and that the rows (representing subjects) align. This requires that both data frames have ‘subject_id’ as a column name. Once this is confirmed, exclude rows with NaN values and separate the brain features and covariate predictors into their own dataframes, using the commands below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 140,
      "metadata": {
        "id": "epaCOjovPxQj"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>participant_id</th>\n",
              "      <th>lh_euler</th>\n",
              "      <th>rh_euler</th>\n",
              "      <th>site</th>\n",
              "      <th>avg_euler</th>\n",
              "      <th>site_median</th>\n",
              "      <th>avg_euler_centered</th>\n",
              "      <th>avg_euler_centered_neg</th>\n",
              "      <th>avg_euler_centered_neg_sqrt</th>\n",
              "      <th>lh_bankssts_thickness</th>\n",
              "      <th>lh_caudalanteriorcingulate_thickness</th>\n",
              "      <th>lh_caudalmiddlefrontal_thickness</th>\n",
              "      <th>lh_cuneus_thickness</th>\n",
              "      <th>lh_entorhinal_thickness</th>\n",
              "      <th>lh_fusiform_thickness</th>\n",
              "      <th>lh_inferiorparietal_thickness</th>\n",
              "      <th>lh_inferiortemporal_thickness</th>\n",
              "      <th>lh_isthmuscingulate_thickness</th>\n",
              "      <th>lh_lateraloccipital_thickness</th>\n",
              "      <th>lh_lateralorbitofrontal_thickness</th>\n",
              "      <th>lh_lingual_thickness</th>\n",
              "      <th>lh_medialorbitofrontal_thickness</th>\n",
              "      <th>lh_middletemporal_thickness</th>\n",
              "      <th>lh_parahippocampal_thickness</th>\n",
              "      <th>lh_paracentral_thickness</th>\n",
              "      <th>lh_parsopercularis_thickness</th>\n",
              "      <th>lh_parsorbitalis_thickness</th>\n",
              "      <th>lh_parstriangularis_thickness</th>\n",
              "      <th>lh_pericalcarine_thickness</th>\n",
              "      <th>lh_postcentral_thickness</th>\n",
              "      <th>lh_posteriorcingulate_thickness</th>\n",
              "      <th>lh_precentral_thickness</th>\n",
              "      <th>lh_precuneus_thickness</th>\n",
              "      <th>lh_rostralanteriorcingulate_thickness</th>\n",
              "      <th>lh_rostralmiddlefrontal_thickness</th>\n",
              "      <th>lh_superiorfrontal_thickness</th>\n",
              "      <th>lh_superiorparietal_thickness</th>\n",
              "      <th>lh_superiortemporal_thickness</th>\n",
              "      <th>lh_supramarginal_thickness</th>\n",
              "      <th>lh_frontalpole_thickness</th>\n",
              "      <th>lh_temporalpole_thickness</th>\n",
              "      <th>lh_transversetemporal_thickness</th>\n",
              "      <th>lh_insula_thickness</th>\n",
              "      <th>lh_MeanThickness_thickness</th>\n",
              "      <th>BrainSegVolNotVent</th>\n",
              "      <th>eTIV</th>\n",
              "      <th>rh_bankssts_thickness</th>\n",
              "      <th>rh_caudalanteriorcingulate_thickness</th>\n",
              "      <th>rh_caudalmiddlefrontal_thickness</th>\n",
              "      <th>rh_cuneus_thickness</th>\n",
              "      <th>rh_entorhinal_thickness</th>\n",
              "      <th>rh_fusiform_thickness</th>\n",
              "      <th>rh_inferiorparietal_thickness</th>\n",
              "      <th>rh_inferiortemporal_thickness</th>\n",
              "      <th>rh_isthmuscingulate_thickness</th>\n",
              "      <th>rh_lateraloccipital_thickness</th>\n",
              "      <th>rh_lateralorbitofrontal_thickness</th>\n",
              "      <th>rh_lingual_thickness</th>\n",
              "      <th>rh_medialorbitofrontal_thickness</th>\n",
              "      <th>rh_middletemporal_thickness</th>\n",
              "      <th>rh_parahippocampal_thickness</th>\n",
              "      <th>rh_paracentral_thickness</th>\n",
              "      <th>rh_parsopercularis_thickness</th>\n",
              "      <th>rh_parsorbitalis_thickness</th>\n",
              "      <th>rh_parstriangularis_thickness</th>\n",
              "      <th>rh_pericalcarine_thickness</th>\n",
              "      <th>rh_postcentral_thickness</th>\n",
              "      <th>rh_posteriorcingulate_thickness</th>\n",
              "      <th>rh_precentral_thickness</th>\n",
              "      <th>rh_precuneus_thickness</th>\n",
              "      <th>rh_rostralanteriorcingulate_thickness</th>\n",
              "      <th>rh_rostralmiddlefrontal_thickness</th>\n",
              "      <th>rh_superiorfrontal_thickness</th>\n",
              "      <th>rh_superiorparietal_thickness</th>\n",
              "      <th>rh_superiortemporal_thickness</th>\n",
              "      <th>rh_supramarginal_thickness</th>\n",
              "      <th>rh_frontalpole_thickness</th>\n",
              "      <th>rh_temporalpole_thickness</th>\n",
              "      <th>rh_transversetemporal_thickness</th>\n",
              "      <th>rh_insula_thickness</th>\n",
              "      <th>rh_MeanThickness_thickness</th>\n",
              "      <th>age</th>\n",
              "      <th>sex</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>sub-100206</td>\n",
              "      <td>-62</td>\n",
              "      <td>-50</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-56.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.605551</td>\n",
              "      <td>2.733</td>\n",
              "      <td>2.682</td>\n",
              "      <td>2.765</td>\n",
              "      <td>2.185</td>\n",
              "      <td>3.388</td>\n",
              "      <td>2.957</td>\n",
              "      <td>2.583</td>\n",
              "      <td>3.065</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.304</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.573</td>\n",
              "      <td>3.011</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.822</td>\n",
              "      <td>2.719</td>\n",
              "      <td>2.562</td>\n",
              "      <td>2.199</td>\n",
              "      <td>2.259</td>\n",
              "      <td>2.610</td>\n",
              "      <td>2.661</td>\n",
              "      <td>2.479</td>\n",
              "      <td>3.102</td>\n",
              "      <td>2.580</td>\n",
              "      <td>2.827</td>\n",
              "      <td>2.235</td>\n",
              "      <td>3.017</td>\n",
              "      <td>2.701</td>\n",
              "      <td>2.780</td>\n",
              "      <td>3.237</td>\n",
              "      <td>2.677</td>\n",
              "      <td>3.198</td>\n",
              "      <td>2.63693</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.744</td>\n",
              "      <td>2.505</td>\n",
              "      <td>2.713</td>\n",
              "      <td>2.091</td>\n",
              "      <td>3.350</td>\n",
              "      <td>2.683</td>\n",
              "      <td>2.592</td>\n",
              "      <td>2.960</td>\n",
              "      <td>2.313</td>\n",
              "      <td>2.299</td>\n",
              "      <td>2.737</td>\n",
              "      <td>2.094</td>\n",
              "      <td>2.620</td>\n",
              "      <td>2.987</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.377</td>\n",
              "      <td>2.823</td>\n",
              "      <td>2.967</td>\n",
              "      <td>2.624</td>\n",
              "      <td>2.160</td>\n",
              "      <td>2.215</td>\n",
              "      <td>2.505</td>\n",
              "      <td>2.632</td>\n",
              "      <td>2.444</td>\n",
              "      <td>3.115</td>\n",
              "      <td>2.515</td>\n",
              "      <td>2.731</td>\n",
              "      <td>2.278</td>\n",
              "      <td>3.032</td>\n",
              "      <td>2.660</td>\n",
              "      <td>2.641</td>\n",
              "      <td>3.579</td>\n",
              "      <td>3.147</td>\n",
              "      <td>3.278</td>\n",
              "      <td>2.58316</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>sub-100307</td>\n",
              "      <td>-38</td>\n",
              "      <td>-90</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-64.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>-21.0</td>\n",
              "      <td>21.0</td>\n",
              "      <td>4.582576</td>\n",
              "      <td>2.713</td>\n",
              "      <td>3.253</td>\n",
              "      <td>2.795</td>\n",
              "      <td>1.957</td>\n",
              "      <td>3.389</td>\n",
              "      <td>3.062</td>\n",
              "      <td>2.637</td>\n",
              "      <td>2.968</td>\n",
              "      <td>2.533</td>\n",
              "      <td>2.259</td>\n",
              "      <td>3.155</td>\n",
              "      <td>2.145</td>\n",
              "      <td>2.730</td>\n",
              "      <td>3.075</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.572</td>\n",
              "      <td>2.917</td>\n",
              "      <td>3.165</td>\n",
              "      <td>2.707</td>\n",
              "      <td>1.891</td>\n",
              "      <td>2.225</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.808</td>\n",
              "      <td>2.513</td>\n",
              "      <td>3.057</td>\n",
              "      <td>2.791</td>\n",
              "      <td>2.948</td>\n",
              "      <td>2.247</td>\n",
              "      <td>2.986</td>\n",
              "      <td>2.684</td>\n",
              "      <td>3.649</td>\n",
              "      <td>3.612</td>\n",
              "      <td>2.700</td>\n",
              "      <td>3.349</td>\n",
              "      <td>2.70653</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.879</td>\n",
              "      <td>3.020</td>\n",
              "      <td>2.802</td>\n",
              "      <td>2.117</td>\n",
              "      <td>3.685</td>\n",
              "      <td>2.995</td>\n",
              "      <td>2.625</td>\n",
              "      <td>3.076</td>\n",
              "      <td>2.288</td>\n",
              "      <td>2.403</td>\n",
              "      <td>3.028</td>\n",
              "      <td>2.179</td>\n",
              "      <td>3.123</td>\n",
              "      <td>3.187</td>\n",
              "      <td>2.814</td>\n",
              "      <td>2.587</td>\n",
              "      <td>3.080</td>\n",
              "      <td>2.978</td>\n",
              "      <td>2.793</td>\n",
              "      <td>2.050</td>\n",
              "      <td>2.166</td>\n",
              "      <td>2.743</td>\n",
              "      <td>2.758</td>\n",
              "      <td>2.601</td>\n",
              "      <td>3.012</td>\n",
              "      <td>2.789</td>\n",
              "      <td>3.087</td>\n",
              "      <td>2.250</td>\n",
              "      <td>3.151</td>\n",
              "      <td>2.766</td>\n",
              "      <td>3.675</td>\n",
              "      <td>4.026</td>\n",
              "      <td>2.819</td>\n",
              "      <td>3.002</td>\n",
              "      <td>2.74140</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>sub-100408</td>\n",
              "      <td>-70</td>\n",
              "      <td>-42</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-56.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>-13.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>3.605551</td>\n",
              "      <td>2.532</td>\n",
              "      <td>2.588</td>\n",
              "      <td>2.612</td>\n",
              "      <td>2.127</td>\n",
              "      <td>3.310</td>\n",
              "      <td>2.870</td>\n",
              "      <td>2.515</td>\n",
              "      <td>2.840</td>\n",
              "      <td>2.257</td>\n",
              "      <td>2.247</td>\n",
              "      <td>2.715</td>\n",
              "      <td>2.294</td>\n",
              "      <td>2.530</td>\n",
              "      <td>2.868</td>\n",
              "      <td>2.210</td>\n",
              "      <td>2.489</td>\n",
              "      <td>2.579</td>\n",
              "      <td>2.790</td>\n",
              "      <td>2.502</td>\n",
              "      <td>1.919</td>\n",
              "      <td>2.191</td>\n",
              "      <td>2.575</td>\n",
              "      <td>2.610</td>\n",
              "      <td>2.498</td>\n",
              "      <td>2.833</td>\n",
              "      <td>2.525</td>\n",
              "      <td>2.730</td>\n",
              "      <td>2.311</td>\n",
              "      <td>2.887</td>\n",
              "      <td>2.567</td>\n",
              "      <td>2.748</td>\n",
              "      <td>3.344</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.968</td>\n",
              "      <td>2.56161</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.232</td>\n",
              "      <td>2.552</td>\n",
              "      <td>2.173</td>\n",
              "      <td>3.232</td>\n",
              "      <td>2.816</td>\n",
              "      <td>2.582</td>\n",
              "      <td>2.857</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.272</td>\n",
              "      <td>2.905</td>\n",
              "      <td>2.223</td>\n",
              "      <td>2.762</td>\n",
              "      <td>2.991</td>\n",
              "      <td>2.423</td>\n",
              "      <td>2.536</td>\n",
              "      <td>2.820</td>\n",
              "      <td>2.742</td>\n",
              "      <td>2.524</td>\n",
              "      <td>2.056</td>\n",
              "      <td>2.214</td>\n",
              "      <td>2.481</td>\n",
              "      <td>2.646</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.862</td>\n",
              "      <td>2.502</td>\n",
              "      <td>2.760</td>\n",
              "      <td>2.282</td>\n",
              "      <td>2.846</td>\n",
              "      <td>2.554</td>\n",
              "      <td>2.840</td>\n",
              "      <td>3.102</td>\n",
              "      <td>2.532</td>\n",
              "      <td>2.947</td>\n",
              "      <td>2.57804</td>\n",
              "      <td>33.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>sub-100610</td>\n",
              "      <td>-30</td>\n",
              "      <td>-38</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-34.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>9.0</td>\n",
              "      <td>-9.0</td>\n",
              "      <td>3.000000</td>\n",
              "      <td>2.683</td>\n",
              "      <td>3.029</td>\n",
              "      <td>2.735</td>\n",
              "      <td>2.223</td>\n",
              "      <td>3.664</td>\n",
              "      <td>2.873</td>\n",
              "      <td>2.547</td>\n",
              "      <td>3.062</td>\n",
              "      <td>2.471</td>\n",
              "      <td>2.238</td>\n",
              "      <td>2.881</td>\n",
              "      <td>2.149</td>\n",
              "      <td>2.412</td>\n",
              "      <td>3.014</td>\n",
              "      <td>3.324</td>\n",
              "      <td>2.518</td>\n",
              "      <td>2.849</td>\n",
              "      <td>2.865</td>\n",
              "      <td>2.886</td>\n",
              "      <td>2.009</td>\n",
              "      <td>2.233</td>\n",
              "      <td>2.556</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.512</td>\n",
              "      <td>3.153</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.947</td>\n",
              "      <td>2.272</td>\n",
              "      <td>2.870</td>\n",
              "      <td>2.671</td>\n",
              "      <td>3.020</td>\n",
              "      <td>3.427</td>\n",
              "      <td>2.591</td>\n",
              "      <td>3.156</td>\n",
              "      <td>2.65715</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.743</td>\n",
              "      <td>2.623</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.120</td>\n",
              "      <td>3.737</td>\n",
              "      <td>2.912</td>\n",
              "      <td>2.578</td>\n",
              "      <td>2.800</td>\n",
              "      <td>2.381</td>\n",
              "      <td>2.300</td>\n",
              "      <td>2.872</td>\n",
              "      <td>2.304</td>\n",
              "      <td>2.900</td>\n",
              "      <td>3.013</td>\n",
              "      <td>2.775</td>\n",
              "      <td>2.536</td>\n",
              "      <td>2.779</td>\n",
              "      <td>2.842</td>\n",
              "      <td>2.851</td>\n",
              "      <td>2.142</td>\n",
              "      <td>2.154</td>\n",
              "      <td>2.528</td>\n",
              "      <td>2.750</td>\n",
              "      <td>2.512</td>\n",
              "      <td>3.336</td>\n",
              "      <td>2.620</td>\n",
              "      <td>3.013</td>\n",
              "      <td>2.257</td>\n",
              "      <td>2.901</td>\n",
              "      <td>2.632</td>\n",
              "      <td>2.994</td>\n",
              "      <td>3.723</td>\n",
              "      <td>2.721</td>\n",
              "      <td>3.068</td>\n",
              "      <td>2.64460</td>\n",
              "      <td>27.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>sub-101006</td>\n",
              "      <td>-16</td>\n",
              "      <td>-24</td>\n",
              "      <td>hcp</td>\n",
              "      <td>-20.0</td>\n",
              "      <td>-43</td>\n",
              "      <td>23.0</td>\n",
              "      <td>-23.0</td>\n",
              "      <td>4.795832</td>\n",
              "      <td>2.632</td>\n",
              "      <td>2.657</td>\n",
              "      <td>2.839</td>\n",
              "      <td>1.977</td>\n",
              "      <td>3.236</td>\n",
              "      <td>2.841</td>\n",
              "      <td>2.522</td>\n",
              "      <td>2.846</td>\n",
              "      <td>2.300</td>\n",
              "      <td>2.231</td>\n",
              "      <td>2.658</td>\n",
              "      <td>1.946</td>\n",
              "      <td>2.637</td>\n",
              "      <td>2.910</td>\n",
              "      <td>2.616</td>\n",
              "      <td>2.487</td>\n",
              "      <td>2.899</td>\n",
              "      <td>2.729</td>\n",
              "      <td>2.587</td>\n",
              "      <td>2.043</td>\n",
              "      <td>2.099</td>\n",
              "      <td>2.441</td>\n",
              "      <td>2.861</td>\n",
              "      <td>2.341</td>\n",
              "      <td>3.004</td>\n",
              "      <td>2.529</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.204</td>\n",
              "      <td>2.858</td>\n",
              "      <td>2.535</td>\n",
              "      <td>2.942</td>\n",
              "      <td>3.211</td>\n",
              "      <td>2.791</td>\n",
              "      <td>2.995</td>\n",
              "      <td>2.55864</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>2.742</td>\n",
              "      <td>1.890</td>\n",
              "      <td>2.866</td>\n",
              "      <td>2.065</td>\n",
              "      <td>3.234</td>\n",
              "      <td>2.867</td>\n",
              "      <td>2.516</td>\n",
              "      <td>2.833</td>\n",
              "      <td>1.951</td>\n",
              "      <td>2.229</td>\n",
              "      <td>2.878</td>\n",
              "      <td>2.007</td>\n",
              "      <td>2.626</td>\n",
              "      <td>2.981</td>\n",
              "      <td>2.836</td>\n",
              "      <td>2.581</td>\n",
              "      <td>2.960</td>\n",
              "      <td>2.669</td>\n",
              "      <td>2.639</td>\n",
              "      <td>1.993</td>\n",
              "      <td>2.156</td>\n",
              "      <td>2.008</td>\n",
              "      <td>2.771</td>\n",
              "      <td>2.543</td>\n",
              "      <td>2.807</td>\n",
              "      <td>2.581</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.313</td>\n",
              "      <td>2.879</td>\n",
              "      <td>2.674</td>\n",
              "      <td>2.658</td>\n",
              "      <td>3.540</td>\n",
              "      <td>2.726</td>\n",
              "      <td>2.958</td>\n",
              "      <td>2.57177</td>\n",
              "      <td>35.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1687</th>\n",
              "      <td>sub-IXI648</td>\n",
              "      <td>-38</td>\n",
              "      <td>-42</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-40.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>16.0</td>\n",
              "      <td>-16.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>2.697</td>\n",
              "      <td>2.459</td>\n",
              "      <td>2.155</td>\n",
              "      <td>1.756</td>\n",
              "      <td>3.482</td>\n",
              "      <td>2.706</td>\n",
              "      <td>2.428</td>\n",
              "      <td>2.977</td>\n",
              "      <td>2.059</td>\n",
              "      <td>2.290</td>\n",
              "      <td>2.640</td>\n",
              "      <td>1.895</td>\n",
              "      <td>2.380</td>\n",
              "      <td>2.878</td>\n",
              "      <td>2.619</td>\n",
              "      <td>1.966</td>\n",
              "      <td>2.360</td>\n",
              "      <td>2.501</td>\n",
              "      <td>2.283</td>\n",
              "      <td>1.517</td>\n",
              "      <td>1.877</td>\n",
              "      <td>2.683</td>\n",
              "      <td>2.352</td>\n",
              "      <td>2.267</td>\n",
              "      <td>2.596</td>\n",
              "      <td>2.203</td>\n",
              "      <td>2.432</td>\n",
              "      <td>2.057</td>\n",
              "      <td>2.588</td>\n",
              "      <td>2.360</td>\n",
              "      <td>2.133</td>\n",
              "      <td>3.907</td>\n",
              "      <td>1.852</td>\n",
              "      <td>3.073</td>\n",
              "      <td>2.37989</td>\n",
              "      <td>1204414.0</td>\n",
              "      <td>1.685664e+06</td>\n",
              "      <td>2.432</td>\n",
              "      <td>2.298</td>\n",
              "      <td>2.232</td>\n",
              "      <td>1.854</td>\n",
              "      <td>3.643</td>\n",
              "      <td>2.686</td>\n",
              "      <td>2.398</td>\n",
              "      <td>2.860</td>\n",
              "      <td>2.491</td>\n",
              "      <td>2.365</td>\n",
              "      <td>2.793</td>\n",
              "      <td>1.904</td>\n",
              "      <td>2.464</td>\n",
              "      <td>2.872</td>\n",
              "      <td>2.451</td>\n",
              "      <td>2.047</td>\n",
              "      <td>2.416</td>\n",
              "      <td>2.636</td>\n",
              "      <td>2.160</td>\n",
              "      <td>1.449</td>\n",
              "      <td>1.886</td>\n",
              "      <td>2.583</td>\n",
              "      <td>2.269</td>\n",
              "      <td>2.254</td>\n",
              "      <td>2.747</td>\n",
              "      <td>2.131</td>\n",
              "      <td>2.390</td>\n",
              "      <td>1.927</td>\n",
              "      <td>2.568</td>\n",
              "      <td>2.254</td>\n",
              "      <td>2.454</td>\n",
              "      <td>3.595</td>\n",
              "      <td>1.991</td>\n",
              "      <td>2.977</td>\n",
              "      <td>2.34768</td>\n",
              "      <td>47.723477</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1688</th>\n",
              "      <td>sub-IXI651</td>\n",
              "      <td>-34</td>\n",
              "      <td>-30</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-32.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>24.0</td>\n",
              "      <td>-24.0</td>\n",
              "      <td>4.898979</td>\n",
              "      <td>2.529</td>\n",
              "      <td>3.073</td>\n",
              "      <td>2.675</td>\n",
              "      <td>2.124</td>\n",
              "      <td>4.006</td>\n",
              "      <td>2.864</td>\n",
              "      <td>2.675</td>\n",
              "      <td>2.969</td>\n",
              "      <td>2.413</td>\n",
              "      <td>2.211</td>\n",
              "      <td>2.930</td>\n",
              "      <td>2.046</td>\n",
              "      <td>2.634</td>\n",
              "      <td>3.128</td>\n",
              "      <td>3.233</td>\n",
              "      <td>2.382</td>\n",
              "      <td>2.546</td>\n",
              "      <td>2.640</td>\n",
              "      <td>2.606</td>\n",
              "      <td>1.688</td>\n",
              "      <td>1.934</td>\n",
              "      <td>2.765</td>\n",
              "      <td>2.541</td>\n",
              "      <td>2.446</td>\n",
              "      <td>2.918</td>\n",
              "      <td>2.560</td>\n",
              "      <td>2.810</td>\n",
              "      <td>2.160</td>\n",
              "      <td>2.881</td>\n",
              "      <td>2.479</td>\n",
              "      <td>2.535</td>\n",
              "      <td>3.859</td>\n",
              "      <td>2.372</td>\n",
              "      <td>3.251</td>\n",
              "      <td>2.57618</td>\n",
              "      <td>1036518.0</td>\n",
              "      <td>1.357712e+06</td>\n",
              "      <td>2.539</td>\n",
              "      <td>3.135</td>\n",
              "      <td>2.692</td>\n",
              "      <td>1.766</td>\n",
              "      <td>3.721</td>\n",
              "      <td>2.868</td>\n",
              "      <td>2.631</td>\n",
              "      <td>2.944</td>\n",
              "      <td>2.433</td>\n",
              "      <td>2.333</td>\n",
              "      <td>2.635</td>\n",
              "      <td>2.127</td>\n",
              "      <td>2.539</td>\n",
              "      <td>3.063</td>\n",
              "      <td>3.312</td>\n",
              "      <td>2.167</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.876</td>\n",
              "      <td>2.657</td>\n",
              "      <td>1.662</td>\n",
              "      <td>1.916</td>\n",
              "      <td>2.687</td>\n",
              "      <td>2.267</td>\n",
              "      <td>2.280</td>\n",
              "      <td>3.515</td>\n",
              "      <td>2.535</td>\n",
              "      <td>2.694</td>\n",
              "      <td>2.073</td>\n",
              "      <td>2.935</td>\n",
              "      <td>2.546</td>\n",
              "      <td>2.520</td>\n",
              "      <td>3.997</td>\n",
              "      <td>2.142</td>\n",
              "      <td>3.167</td>\n",
              "      <td>2.51866</td>\n",
              "      <td>50.395619</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1689</th>\n",
              "      <td>sub-IXI652</td>\n",
              "      <td>-38</td>\n",
              "      <td>-54</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-46.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>10.0</td>\n",
              "      <td>-10.0</td>\n",
              "      <td>3.162278</td>\n",
              "      <td>2.499</td>\n",
              "      <td>2.725</td>\n",
              "      <td>2.587</td>\n",
              "      <td>1.670</td>\n",
              "      <td>3.921</td>\n",
              "      <td>2.891</td>\n",
              "      <td>2.444</td>\n",
              "      <td>2.995</td>\n",
              "      <td>2.641</td>\n",
              "      <td>2.172</td>\n",
              "      <td>2.698</td>\n",
              "      <td>1.869</td>\n",
              "      <td>2.560</td>\n",
              "      <td>2.985</td>\n",
              "      <td>2.570</td>\n",
              "      <td>2.138</td>\n",
              "      <td>2.608</td>\n",
              "      <td>2.699</td>\n",
              "      <td>2.528</td>\n",
              "      <td>1.487</td>\n",
              "      <td>2.027</td>\n",
              "      <td>2.478</td>\n",
              "      <td>2.562</td>\n",
              "      <td>2.454</td>\n",
              "      <td>3.044</td>\n",
              "      <td>2.426</td>\n",
              "      <td>2.722</td>\n",
              "      <td>2.115</td>\n",
              "      <td>2.822</td>\n",
              "      <td>2.587</td>\n",
              "      <td>2.721</td>\n",
              "      <td>4.085</td>\n",
              "      <td>2.410</td>\n",
              "      <td>3.116</td>\n",
              "      <td>2.51061</td>\n",
              "      <td>1305959.0</td>\n",
              "      <td>1.715795e+06</td>\n",
              "      <td>2.549</td>\n",
              "      <td>2.890</td>\n",
              "      <td>2.675</td>\n",
              "      <td>1.663</td>\n",
              "      <td>3.511</td>\n",
              "      <td>2.841</td>\n",
              "      <td>2.486</td>\n",
              "      <td>2.934</td>\n",
              "      <td>2.723</td>\n",
              "      <td>2.255</td>\n",
              "      <td>2.797</td>\n",
              "      <td>1.876</td>\n",
              "      <td>2.679</td>\n",
              "      <td>2.845</td>\n",
              "      <td>2.702</td>\n",
              "      <td>2.273</td>\n",
              "      <td>2.803</td>\n",
              "      <td>2.800</td>\n",
              "      <td>2.500</td>\n",
              "      <td>1.500</td>\n",
              "      <td>2.012</td>\n",
              "      <td>2.429</td>\n",
              "      <td>2.565</td>\n",
              "      <td>2.325</td>\n",
              "      <td>2.826</td>\n",
              "      <td>2.480</td>\n",
              "      <td>2.802</td>\n",
              "      <td>1.969</td>\n",
              "      <td>2.997</td>\n",
              "      <td>2.683</td>\n",
              "      <td>2.654</td>\n",
              "      <td>4.229</td>\n",
              "      <td>2.416</td>\n",
              "      <td>3.353</td>\n",
              "      <td>2.51731</td>\n",
              "      <td>42.989733</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1690</th>\n",
              "      <td>sub-IXI653</td>\n",
              "      <td>-58</td>\n",
              "      <td>-54</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-56.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>0.0</td>\n",
              "      <td>-0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.825</td>\n",
              "      <td>2.756</td>\n",
              "      <td>2.518</td>\n",
              "      <td>1.858</td>\n",
              "      <td>3.421</td>\n",
              "      <td>3.018</td>\n",
              "      <td>2.758</td>\n",
              "      <td>3.153</td>\n",
              "      <td>2.273</td>\n",
              "      <td>2.420</td>\n",
              "      <td>2.944</td>\n",
              "      <td>2.004</td>\n",
              "      <td>2.697</td>\n",
              "      <td>3.000</td>\n",
              "      <td>3.279</td>\n",
              "      <td>2.127</td>\n",
              "      <td>2.635</td>\n",
              "      <td>3.073</td>\n",
              "      <td>2.508</td>\n",
              "      <td>1.437</td>\n",
              "      <td>1.979</td>\n",
              "      <td>2.440</td>\n",
              "      <td>2.238</td>\n",
              "      <td>2.416</td>\n",
              "      <td>2.899</td>\n",
              "      <td>2.577</td>\n",
              "      <td>2.682</td>\n",
              "      <td>2.053</td>\n",
              "      <td>3.014</td>\n",
              "      <td>2.580</td>\n",
              "      <td>2.962</td>\n",
              "      <td>4.065</td>\n",
              "      <td>2.387</td>\n",
              "      <td>3.234</td>\n",
              "      <td>2.55112</td>\n",
              "      <td>1272881.0</td>\n",
              "      <td>1.663991e+06</td>\n",
              "      <td>2.802</td>\n",
              "      <td>2.378</td>\n",
              "      <td>2.369</td>\n",
              "      <td>1.885</td>\n",
              "      <td>3.901</td>\n",
              "      <td>2.844</td>\n",
              "      <td>2.782</td>\n",
              "      <td>3.024</td>\n",
              "      <td>2.468</td>\n",
              "      <td>2.414</td>\n",
              "      <td>2.976</td>\n",
              "      <td>2.090</td>\n",
              "      <td>2.745</td>\n",
              "      <td>3.011</td>\n",
              "      <td>3.187</td>\n",
              "      <td>2.018</td>\n",
              "      <td>2.646</td>\n",
              "      <td>2.804</td>\n",
              "      <td>2.527</td>\n",
              "      <td>1.668</td>\n",
              "      <td>1.826</td>\n",
              "      <td>2.528</td>\n",
              "      <td>2.250</td>\n",
              "      <td>2.390</td>\n",
              "      <td>3.174</td>\n",
              "      <td>2.460</td>\n",
              "      <td>2.669</td>\n",
              "      <td>2.049</td>\n",
              "      <td>2.784</td>\n",
              "      <td>2.637</td>\n",
              "      <td>2.933</td>\n",
              "      <td>3.881</td>\n",
              "      <td>2.313</td>\n",
              "      <td>3.348</td>\n",
              "      <td>2.51458</td>\n",
              "      <td>46.220397</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1691</th>\n",
              "      <td>sub-IXI662</td>\n",
              "      <td>-32</td>\n",
              "      <td>-26</td>\n",
              "      <td>ixi</td>\n",
              "      <td>-29.0</td>\n",
              "      <td>-56</td>\n",
              "      <td>27.0</td>\n",
              "      <td>-27.0</td>\n",
              "      <td>5.196152</td>\n",
              "      <td>2.478</td>\n",
              "      <td>3.011</td>\n",
              "      <td>2.534</td>\n",
              "      <td>1.823</td>\n",
              "      <td>3.579</td>\n",
              "      <td>3.030</td>\n",
              "      <td>2.365</td>\n",
              "      <td>2.949</td>\n",
              "      <td>2.205</td>\n",
              "      <td>2.229</td>\n",
              "      <td>2.926</td>\n",
              "      <td>1.980</td>\n",
              "      <td>2.659</td>\n",
              "      <td>3.009</td>\n",
              "      <td>3.102</td>\n",
              "      <td>2.145</td>\n",
              "      <td>2.548</td>\n",
              "      <td>3.204</td>\n",
              "      <td>2.682</td>\n",
              "      <td>1.562</td>\n",
              "      <td>2.113</td>\n",
              "      <td>2.739</td>\n",
              "      <td>2.531</td>\n",
              "      <td>2.232</td>\n",
              "      <td>2.992</td>\n",
              "      <td>2.549</td>\n",
              "      <td>2.600</td>\n",
              "      <td>2.109</td>\n",
              "      <td>2.921</td>\n",
              "      <td>2.488</td>\n",
              "      <td>3.119</td>\n",
              "      <td>3.667</td>\n",
              "      <td>2.170</td>\n",
              "      <td>3.415</td>\n",
              "      <td>2.53004</td>\n",
              "      <td>1301484.0</td>\n",
              "      <td>1.726568e+06</td>\n",
              "      <td>2.600</td>\n",
              "      <td>2.653</td>\n",
              "      <td>2.502</td>\n",
              "      <td>1.854</td>\n",
              "      <td>3.561</td>\n",
              "      <td>3.055</td>\n",
              "      <td>2.406</td>\n",
              "      <td>2.916</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.337</td>\n",
              "      <td>3.021</td>\n",
              "      <td>2.083</td>\n",
              "      <td>2.692</td>\n",
              "      <td>3.031</td>\n",
              "      <td>2.822</td>\n",
              "      <td>2.137</td>\n",
              "      <td>2.684</td>\n",
              "      <td>2.906</td>\n",
              "      <td>2.627</td>\n",
              "      <td>1.709</td>\n",
              "      <td>2.058</td>\n",
              "      <td>2.763</td>\n",
              "      <td>2.386</td>\n",
              "      <td>2.445</td>\n",
              "      <td>3.139</td>\n",
              "      <td>2.636</td>\n",
              "      <td>2.725</td>\n",
              "      <td>2.037</td>\n",
              "      <td>2.911</td>\n",
              "      <td>2.545</td>\n",
              "      <td>2.812</td>\n",
              "      <td>4.012</td>\n",
              "      <td>2.485</td>\n",
              "      <td>3.295</td>\n",
              "      <td>2.55809</td>\n",
              "      <td>41.741273</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>1692 rows × 83 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     participant_id  lh_euler  rh_euler site  avg_euler  site_median  avg_euler_centered  avg_euler_centered_neg  avg_euler_centered_neg_sqrt  lh_bankssts_thickness  lh_caudalanteriorcingulate_thickness  lh_caudalmiddlefrontal_thickness  lh_cuneus_thickness  lh_entorhinal_thickness  lh_fusiform_thickness  lh_inferiorparietal_thickness  lh_inferiortemporal_thickness  lh_isthmuscingulate_thickness  lh_lateraloccipital_thickness  lh_lateralorbitofrontal_thickness  lh_lingual_thickness  lh_medialorbitofrontal_thickness  lh_middletemporal_thickness  lh_parahippocampal_thickness  lh_paracentral_thickness  lh_parsopercularis_thickness  lh_parsorbitalis_thickness  lh_parstriangularis_thickness  lh_pericalcarine_thickness  lh_postcentral_thickness  lh_posteriorcingulate_thickness  lh_precentral_thickness  lh_precuneus_thickness  lh_rostralanteriorcingulate_thickness  lh_rostralmiddlefrontal_thickness  lh_superiorfrontal_thickness  lh_superiorparietal_thickness  lh_superiortemporal_thickness  \\\n",
              "0        sub-100206       -62       -50  hcp      -56.0          -43               -13.0                    13.0                     3.605551                  2.733                                 2.682                             2.765                2.185                    3.388                  2.957                          2.583                          3.065                          2.428                          2.304                              2.916                 2.259                             2.573                        3.011                         2.916                     2.487                         2.822                       2.719                          2.562                       2.199                     2.259                            2.610                    2.661                   2.479                                  3.102                              2.580                         2.827                          2.235                          3.017   \n",
              "1        sub-100307       -38       -90  hcp      -64.0          -43               -21.0                    21.0                     4.582576                  2.713                                 3.253                             2.795                1.957                    3.389                  3.062                          2.637                          2.968                          2.533                          2.259                              3.155                 2.145                             2.730                        3.075                         2.460                     2.572                         2.917                       3.165                          2.707                       1.891                     2.225                            2.864                    2.808                   2.513                                  3.057                              2.791                         2.948                          2.247                          2.986   \n",
              "2        sub-100408       -70       -42  hcp      -56.0          -43               -13.0                    13.0                     3.605551                  2.532                                 2.588                             2.612                2.127                    3.310                  2.870                          2.515                          2.840                          2.257                          2.247                              2.715                 2.294                             2.530                        2.868                         2.210                     2.489                         2.579                       2.790                          2.502                       1.919                     2.191                            2.575                    2.610                   2.498                                  2.833                              2.525                         2.730                          2.311                          2.887   \n",
              "3        sub-100610       -30       -38  hcp      -34.0          -43                 9.0                    -9.0                     3.000000                  2.683                                 3.029                             2.735                2.223                    3.664                  2.873                          2.547                          3.062                          2.471                          2.238                              2.881                 2.149                             2.412                        3.014                         3.324                     2.518                         2.849                       2.865                          2.886                       2.009                     2.233                            2.556                    2.864                   2.512                                  3.153                              2.687                         2.947                          2.272                          2.870   \n",
              "4        sub-101006       -16       -24  hcp      -20.0          -43                23.0                   -23.0                     4.795832                  2.632                                 2.657                             2.839                1.977                    3.236                  2.841                          2.522                          2.846                          2.300                          2.231                              2.658                 1.946                             2.637                        2.910                         2.616                     2.487                         2.899                       2.729                          2.587                       2.043                     2.099                            2.441                    2.861                   2.341                                  3.004                              2.529                         2.864                          2.204                          2.858   \n",
              "...             ...       ...       ...  ...        ...          ...                 ...                     ...                          ...                    ...                                   ...                               ...                  ...                      ...                    ...                            ...                            ...                            ...                            ...                                ...                   ...                               ...                          ...                           ...                       ...                           ...                         ...                            ...                         ...                       ...                              ...                      ...                     ...                                    ...                                ...                           ...                            ...                            ...   \n",
              "1687     sub-IXI648       -38       -42  ixi      -40.0          -56                16.0                   -16.0                     4.000000                  2.697                                 2.459                             2.155                1.756                    3.482                  2.706                          2.428                          2.977                          2.059                          2.290                              2.640                 1.895                             2.380                        2.878                         2.619                     1.966                         2.360                       2.501                          2.283                       1.517                     1.877                            2.683                    2.352                   2.267                                  2.596                              2.203                         2.432                          2.057                          2.588   \n",
              "1688     sub-IXI651       -34       -30  ixi      -32.0          -56                24.0                   -24.0                     4.898979                  2.529                                 3.073                             2.675                2.124                    4.006                  2.864                          2.675                          2.969                          2.413                          2.211                              2.930                 2.046                             2.634                        3.128                         3.233                     2.382                         2.546                       2.640                          2.606                       1.688                     1.934                            2.765                    2.541                   2.446                                  2.918                              2.560                         2.810                          2.160                          2.881   \n",
              "1689     sub-IXI652       -38       -54  ixi      -46.0          -56                10.0                   -10.0                     3.162278                  2.499                                 2.725                             2.587                1.670                    3.921                  2.891                          2.444                          2.995                          2.641                          2.172                              2.698                 1.869                             2.560                        2.985                         2.570                     2.138                         2.608                       2.699                          2.528                       1.487                     2.027                            2.478                    2.562                   2.454                                  3.044                              2.426                         2.722                          2.115                          2.822   \n",
              "1690     sub-IXI653       -58       -54  ixi      -56.0          -56                 0.0                    -0.0                     0.000000                  2.825                                 2.756                             2.518                1.858                    3.421                  3.018                          2.758                          3.153                          2.273                          2.420                              2.944                 2.004                             2.697                        3.000                         3.279                     2.127                         2.635                       3.073                          2.508                       1.437                     1.979                            2.440                    2.238                   2.416                                  2.899                              2.577                         2.682                          2.053                          3.014   \n",
              "1691     sub-IXI662       -32       -26  ixi      -29.0          -56                27.0                   -27.0                     5.196152                  2.478                                 3.011                             2.534                1.823                    3.579                  3.030                          2.365                          2.949                          2.205                          2.229                              2.926                 1.980                             2.659                        3.009                         3.102                     2.145                         2.548                       3.204                          2.682                       1.562                     2.113                            2.739                    2.531                   2.232                                  2.992                              2.549                         2.600                          2.109                          2.921   \n",
              "\n",
              "      lh_supramarginal_thickness  lh_frontalpole_thickness  lh_temporalpole_thickness  lh_transversetemporal_thickness  lh_insula_thickness  lh_MeanThickness_thickness  BrainSegVolNotVent          eTIV  rh_bankssts_thickness  rh_caudalanteriorcingulate_thickness  rh_caudalmiddlefrontal_thickness  rh_cuneus_thickness  rh_entorhinal_thickness  rh_fusiform_thickness  rh_inferiorparietal_thickness  rh_inferiortemporal_thickness  rh_isthmuscingulate_thickness  rh_lateraloccipital_thickness  rh_lateralorbitofrontal_thickness  rh_lingual_thickness  rh_medialorbitofrontal_thickness  rh_middletemporal_thickness  rh_parahippocampal_thickness  rh_paracentral_thickness  rh_parsopercularis_thickness  rh_parsorbitalis_thickness  rh_parstriangularis_thickness  rh_pericalcarine_thickness  rh_postcentral_thickness  rh_posteriorcingulate_thickness  rh_precentral_thickness  rh_precuneus_thickness  rh_rostralanteriorcingulate_thickness  rh_rostralmiddlefrontal_thickness  rh_superiorfrontal_thickness  \\\n",
              "0                          2.701                     2.780                      3.237                            2.677                3.198                     2.63693                 NaN           NaN                  2.744                                 2.505                             2.713                2.091                    3.350                  2.683                          2.592                          2.960                          2.313                          2.299                              2.737                 2.094                             2.620                        2.987                         2.487                     2.377                         2.823                       2.967                          2.624                       2.160                     2.215                            2.505                    2.632                   2.444                                  3.115                              2.515                         2.731   \n",
              "1                          2.684                     3.649                      3.612                            2.700                3.349                     2.70653                 NaN           NaN                  2.879                                 3.020                             2.802                2.117                    3.685                  2.995                          2.625                          3.076                          2.288                          2.403                              3.028                 2.179                             3.123                        3.187                         2.814                     2.587                         3.080                       2.978                          2.793                       2.050                     2.166                            2.743                    2.758                   2.601                                  3.012                              2.789                         3.087   \n",
              "2                          2.567                     2.748                      3.344                            2.556                2.968                     2.56161                 NaN           NaN                  2.729                                 2.232                             2.552                2.173                    3.232                  2.816                          2.582                          2.857                          2.203                          2.272                              2.905                 2.223                             2.762                        2.991                         2.423                     2.536                         2.820                       2.742                          2.524                       2.056                     2.214                            2.481                    2.646                   2.529                                  2.862                              2.502                         2.760   \n",
              "3                          2.671                     3.020                      3.427                            2.591                3.156                     2.65715                 NaN           NaN                  2.743                                 2.623                             2.739                2.120                    3.737                  2.912                          2.578                          2.800                          2.381                          2.300                              2.872                 2.304                             2.900                        3.013                         2.775                     2.536                         2.779                       2.842                          2.851                       2.142                     2.154                            2.528                    2.750                   2.512                                  3.336                              2.620                         3.013   \n",
              "4                          2.535                     2.942                      3.211                            2.791                2.995                     2.55864                 NaN           NaN                  2.742                                 1.890                             2.866                2.065                    3.234                  2.867                          2.516                          2.833                          1.951                          2.229                              2.878                 2.007                             2.626                        2.981                         2.836                     2.581                         2.960                       2.669                          2.639                       1.993                     2.156                            2.008                    2.771                   2.543                                  2.807                              2.581                         2.864   \n",
              "...                          ...                       ...                        ...                              ...                  ...                         ...                 ...           ...                    ...                                   ...                               ...                  ...                      ...                    ...                            ...                            ...                            ...                            ...                                ...                   ...                               ...                          ...                           ...                       ...                           ...                         ...                            ...                         ...                       ...                              ...                      ...                     ...                                    ...                                ...                           ...   \n",
              "1687                       2.360                     2.133                      3.907                            1.852                3.073                     2.37989           1204414.0  1.685664e+06                  2.432                                 2.298                             2.232                1.854                    3.643                  2.686                          2.398                          2.860                          2.491                          2.365                              2.793                 1.904                             2.464                        2.872                         2.451                     2.047                         2.416                       2.636                          2.160                       1.449                     1.886                            2.583                    2.269                   2.254                                  2.747                              2.131                         2.390   \n",
              "1688                       2.479                     2.535                      3.859                            2.372                3.251                     2.57618           1036518.0  1.357712e+06                  2.539                                 3.135                             2.692                1.766                    3.721                  2.868                          2.631                          2.944                          2.433                          2.333                              2.635                 2.127                             2.539                        3.063                         3.312                     2.167                         2.687                       2.876                          2.657                       1.662                     1.916                            2.687                    2.267                   2.280                                  3.515                              2.535                         2.694   \n",
              "1689                       2.587                     2.721                      4.085                            2.410                3.116                     2.51061           1305959.0  1.715795e+06                  2.549                                 2.890                             2.675                1.663                    3.511                  2.841                          2.486                          2.934                          2.723                          2.255                              2.797                 1.876                             2.679                        2.845                         2.702                     2.273                         2.803                       2.800                          2.500                       1.500                     2.012                            2.429                    2.565                   2.325                                  2.826                              2.480                         2.802   \n",
              "1690                       2.580                     2.962                      4.065                            2.387                3.234                     2.55112           1272881.0  1.663991e+06                  2.802                                 2.378                             2.369                1.885                    3.901                  2.844                          2.782                          3.024                          2.468                          2.414                              2.976                 2.090                             2.745                        3.011                         3.187                     2.018                         2.646                       2.804                          2.527                       1.668                     1.826                            2.528                    2.250                   2.390                                  3.174                              2.460                         2.669   \n",
              "1691                       2.488                     3.119                      3.667                            2.170                3.415                     2.53004           1301484.0  1.726568e+06                  2.600                                 2.653                             2.502                1.854                    3.561                  3.055                          2.406                          2.916                          2.386                          2.337                              3.021                 2.083                             2.692                        3.031                         2.822                     2.137                         2.684                       2.906                          2.627                       1.709                     2.058                            2.763                    2.386                   2.445                                  3.139                              2.636                         2.725   \n",
              "\n",
              "      rh_superiorparietal_thickness  rh_superiortemporal_thickness  rh_supramarginal_thickness  rh_frontalpole_thickness  rh_temporalpole_thickness  rh_transversetemporal_thickness  rh_insula_thickness  rh_MeanThickness_thickness        age  sex  \n",
              "0                             2.278                          3.032                       2.660                     2.641                      3.579                            3.147                3.278                     2.58316  27.000000    1  \n",
              "1                             2.250                          3.151                       2.766                     3.675                      4.026                            2.819                3.002                     2.74140  27.000000    2  \n",
              "2                             2.282                          2.846                       2.554                     2.840                      3.102                            2.532                2.947                     2.57804  33.000000    1  \n",
              "3                             2.257                          2.901                       2.632                     2.994                      3.723                            2.721                3.068                     2.64460  27.000000    1  \n",
              "4                             2.313                          2.879                       2.674                     2.658                      3.540                            2.726                2.958                     2.57177  35.000000    2  \n",
              "...                             ...                            ...                         ...                       ...                        ...                              ...                  ...                         ...        ...  ...  \n",
              "1687                          1.927                          2.568                       2.254                     2.454                      3.595                            1.991                2.977                     2.34768  47.723477    1  \n",
              "1688                          2.073                          2.935                       2.546                     2.520                      3.997                            2.142                3.167                     2.51866  50.395619    1  \n",
              "1689                          1.969                          2.997                       2.683                     2.654                      4.229                            2.416                3.353                     2.51731  42.989733    1  \n",
              "1690                          2.049                          2.784                       2.637                     2.933                      3.881                            2.313                3.348                     2.51458  46.220397    1  \n",
              "1691                          2.037                          2.911                       2.545                     2.812                      4.012                            2.485                3.295                     2.55809  41.741273    1  \n",
              "\n",
              "[1692 rows x 83 columns]"
            ]
          },
          "execution_count": 140,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# make sure to use how=\"inner\" so that we only include subjects that have data in both the covariate and the cortical thickness files \n",
        "all_data = pd.merge(brain_good, cov, how='inner')\n",
        "all_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 141,
      "metadata": {
        "id": "OzHN70ywPxQj"
      },
      "outputs": [],
      "source": [
        "# Create a list of all the ROIs you want to run a normative model for (add additional names to this list if you would like to include other brain regions from the Desikan-Killian atlas)\n",
        "roi_ids = ['lh_MeanThickness_thickness',\n",
        "           'rh_MeanThickness_thickness',\n",
        "           'lh_bankssts_thickness',\n",
        "           'lh_caudalanteriorcingulate_thickness',\n",
        "           'lh_superiorfrontal_thickness',\n",
        "           'rh_superiorfrontal_thickness']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 142,
      "metadata": {
        "id": "eEnGF6DmPxQk"
      },
      "outputs": [],
      "source": [
        "# Remove any subjects that have NaN variables in any of the columns\n",
        "all_data.dropna(subset=roi_ids, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 143,
      "metadata": {
        "id": "-x6SldKHPxQk"
      },
      "outputs": [],
      "source": [
        "all_data_features = all_data[roi_ids]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 144,
      "metadata": {
        "id": "aihok464PxQk"
      },
      "outputs": [],
      "source": [
        "all_data_covariates = all_data[['age','sex','site']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VdG79uF2PxQk"
      },
      "source": [
        "**CRITICAL STEP:** `roi_ids` is a variable that represents which brain areas will be modeled and can be used to select subsets of the data frame if you do not wish to run models for the whole brain."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1BXULRkPxQk"
      },
      "source": [
        "## Add variable to model site/scanner effects"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq8JlwV3PxQk"
      },
      "source": [
        "### Step 5.\t\n",
        "\n",
        "Currently, the different sites are coded in a single column (named ‘site’) and are represented as a string data type. However, the PCNtoolkit requires binary variables. Use the pandas package as follows to address this, which has a built-in function, `pd.get_dummies`, that takes in the string ‘site’ column and dummy encodes the site variable so that there is now a column for each site and the columns contain binary variables (0=not in this site, 1=present in this site)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 145,
      "metadata": {
        "id": "7o0VSqO-PxQk"
      },
      "outputs": [],
      "source": [
        "all_data_covariates = pd.get_dummies(all_data_covariates, columns=['site'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 146,
      "metadata": {
        "id": "E2yuYv0EPxQk"
      },
      "outputs": [],
      "source": [
        "all_data['Average_Thickness'] = all_data[['lh_MeanThickness_thickness','rh_MeanThickness_thickness']].mean(axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVF2d878PxQl"
      },
      "source": [
        "## Train/test split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAs2RTAHPxQl"
      },
      "source": [
        "### Step 6.\t\n",
        "\n",
        "In this example, we use 80% of the data for training and 20% for testing. Please carefully read the experimental design section on train/test split considerations when using your own data in this step. Using a function from scikit-learn (`train_test_split`), stratify the train/test split using the site variable to make sure that the train/test sets both contain data from all sites, using the following commands. Next, confirm that your train and test arrays are the same size (rows), using the following commands. You do not need the same size columns (subjects) in the train and test arrays, but the rows represent the covariate and responses which should be the same across train and test arrays."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 147,
      "metadata": {
        "id": "FQtMYy9WPxQl"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(all_data_covariates, all_data_features, stratify=all_data['site'], test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8kcWH0eUPxQl"
      },
      "source": [
        "Verify that your train & test arrays are the same size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 148,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ln7_zLFWPxQl",
        "outputId": "5710c898-1056-4587-9140-ffc444d10e77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train covariate size is:  (1353, 4)\n",
            "Test covariate size is:  (339, 4)\n",
            "Train response size is:  (1353, 6)\n",
            "Test response size is:  (339, 6)\n"
          ]
        }
      ],
      "source": [
        "tr_cov_size = X_train.shape\n",
        "tr_resp_size = y_train.shape\n",
        "te_cov_size = X_test.shape\n",
        "te_resp_size = y_test.shape\n",
        "print(\"Train covariate size is: \", tr_cov_size)\n",
        "print(\"Test covariate size is: \", te_cov_size)\n",
        "print(\"Train response size is: \", tr_resp_size)\n",
        "print(\"Test response size is: \", te_resp_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EELPVNVsPxQl"
      },
      "source": [
        "**CRITICAL STEP:** The model would not learn the site effects if all the data from one site was only in the test set. Therefore, we stratify the train/test split using the site variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7umzpdIPxQl"
      },
      "source": [
        "### Step 7.\t\n",
        "\n",
        "When the data were split into train and test sets, the row index was not reset. This means that the row index in the train and test data frames still correspond to the full data frame (before splitting the data occurred). The test set row index informs which subjects belong to which site, and this information is needed to evaluate per site performance metrics. Resetting the row index of the train/test data frames fixes this issue. Then extract the site row indices to a list (one list per site) and create a list called `site_names` that is used to decide which sites to evaluate model performance for, as follows:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 149,
      "metadata": {
        "id": "Fb7EC6vQPxQm"
      },
      "outputs": [],
      "source": [
        "X_train.reset_index(drop=True, inplace=True)\n",
        "X_test.reset_index(drop=True, inplace=True)\n",
        "y_train.reset_index(drop=True, inplace=True)\n",
        "y_test.reset_index(drop=True, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 150,
      "metadata": {
        "id": "pVFGtZL6PxQm"
      },
      "outputs": [],
      "source": [
        "# Get indices of all the subejcts in each site so that we can evaluate the test set metrics per site\n",
        "hcp_idx = X_test.index[X_test['site_hcp'] == 1].to_list()\n",
        "ixi_idx = X_test.index[X_test['site_ixi'] == 1].to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 151,
      "metadata": {
        "id": "xJApegDXPxQm"
      },
      "outputs": [],
      "source": [
        "# Save the site indices into a single list\n",
        "sites = [hcp_idx, ixi_idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 152,
      "metadata": {
        "id": "3_pW9zY4PxQm"
      },
      "outputs": [],
      "source": [
        "# Create a list with sites names to use in evaluating per-site metrics\n",
        "site_names = ['hcp', 'ixi']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfF277S4PxQm"
      },
      "source": [
        "## Setup output directories"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYWrxYgSPxQm"
      },
      "source": [
        "## Step 8.\t\n",
        "\n",
        "Save each brain region to its own text file (organized in separate directories) using the following commands, because for each response variable, Y (e.g., brain region) we fit a separate normative model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 153,
      "metadata": {
        "id": "GgGd1hryPxQm"
      },
      "outputs": [],
      "source": [
        "for c in y_train.columns:\n",
        "    y_train[c].to_csv('resp_tr_' + c + '.txt', header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 154,
      "metadata": {
        "id": "ejSNPBANPxQm"
      },
      "outputs": [],
      "source": [
        "X_train.to_csv('cov_tr.txt', sep = '\\t', header=False, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 155,
      "metadata": {
        "id": "ddJsDnrYPxQn"
      },
      "outputs": [],
      "source": [
        "y_train.to_csv('resp_tr.txt', sep = '\\t', header=False, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 156,
      "metadata": {
        "id": "w0QsVwPoPxQn"
      },
      "outputs": [],
      "source": [
        "for c in y_test.columns:\n",
        "    y_test[c].to_csv('resp_te_' + c + '.txt', header=False, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "metadata": {
        "id": "cv43VtXWPxQn"
      },
      "outputs": [],
      "source": [
        "X_test.to_csv('cov_te.txt', sep = '\\t', header=False, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 158,
      "metadata": {
        "id": "5_6DoEFbPxQn"
      },
      "outputs": [],
      "source": [
        "y_test.to_csv('resp_te.txt', sep = '\\t', header=False, index = False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 159,
      "metadata": {
        "id": "Mnu0sJrfPxQn"
      },
      "outputs": [],
      "source": [
        "! if [[ ! -e ROI_models/ ]]; then mkdir ROI_models; fi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 160,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pl-fYNZsPxQn",
        "outputId": "3a09907a-6ed5-4cd9-fa71-ad889c619e0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘lh_MeanThickness_thickness’: File exists\n",
            "mkdir: cannot create directory ‘lh_bankssts_thickness’: File exists\n",
            "mkdir: cannot create directory ‘lh_caudalanteriorcingulate_thickness’: File exists\n",
            "mkdir: cannot create directory ‘lh_superiorfrontal_thickness’: File exists\n",
            "mkdir: cannot create directory ‘rh_MeanThickness_thickness’: File exists\n",
            "mkdir: cannot create directory ‘rh_superiorfrontal_thickness’: File exists\n"
          ]
        }
      ],
      "source": [
        "! for i in `cat /home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/data/roi_dir_names`; do if [[ -e resp_tr_${i}.txt ]]; then cd ROI_models; mkdir ${i}; cd ../; cp resp_tr_${i}.txt ROI_models/${i}/resp_tr.txt; cp resp_te_${i}.txt ROI_models/${i}/resp_te.txt; cp cov_tr.txt ROI_models/${i}/cov_tr.txt; cp cov_te.txt ROI_models/${i}/cov_te.txt; fi; done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {
        "id": "csrMx397PxQn"
      },
      "outputs": [],
      "source": [
        "# clean up files\n",
        "! rm resp_*.txt "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 162,
      "metadata": {
        "id": "_yhGxsIzPxQn"
      },
      "outputs": [],
      "source": [
        "# clean up files\n",
        "! rm cov_t*.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrOvCLK5PxQn"
      },
      "source": [
        "# Algorithm & Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7v4JoP2PxQn"
      },
      "source": [
        "## Basis expansion using B-Splines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dop9WUXnPxQo"
      },
      "source": [
        "### Step 9.\t\n",
        "\n",
        "Now, set up a B-spline basis set that allows us to perform nonlinear regression using a linear model, using the following commands. This basis is deliberately chosen to not to be too flexible so that it can only model relatively slowly varying trends. To increase the flexibility of the model you can change the parameterization (e.g., by adding knot points to the B-spline basis or increasing the order of the interpolating polynomial). Note that in the neuroimaging literature, it is more common to use a polynomial basis expansion for this. Piecewise polynomials like B-splines are superior to polynomial basis expansions because they do not introduce a global curvature. For further details on the use of B-splines see [Fraza et al](https://pubmed.ncbi.nlm.nih.gov/34798518/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 163,
      "metadata": {
        "id": "XXPJd2BQutvZ"
      },
      "outputs": [],
      "source": [
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 164,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xZni-v2wPxQo",
        "outputId": "134b3f4e-af07-43c0-de8c-9e157dbf52c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Creating basis expansion for ROI: lh_MeanThickness_thickness\n",
            "Creating basis expansion for ROI: rh_MeanThickness_thickness\n",
            "Creating basis expansion for ROI: lh_bankssts_thickness\n",
            "Creating basis expansion for ROI: lh_caudalanteriorcingulate_thickness\n",
            "Creating basis expansion for ROI: lh_superiorfrontal_thickness\n",
            "Creating basis expansion for ROI: rh_superiorfrontal_thickness\n"
          ]
        }
      ],
      "source": [
        "# set this path to wherever your ROI_models folder is located (where you copied all of the covariate & response text files to in Step 4)\n",
        "#data_dir = '/content/PCNtoolkit-demo/tutorials/BLR_protocol/ROI_models/'\n",
        "data_dir = '/home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/tutorials/BLR_protocol/ROI_models/'\n",
        "# Create a cubic B-spline basis (used for regression)\n",
        "xmin = 10#16 # xmin & xmax are the boundaries for ages of participants in the dataset\n",
        "xmax = 95#90\n",
        "B = create_bspline_basis(xmin, xmax)\n",
        "# create the basis expansion for the covariates for each of the \n",
        "for roi in roi_ids: \n",
        "    print('Creating basis expansion for ROI:', roi)\n",
        "    roi_dir = os.path.join(data_dir, roi)\n",
        "    os.chdir(roi_dir)\n",
        "    # create output dir \n",
        "    os.makedirs(os.path.join(roi_dir,'blr'), exist_ok=True)\n",
        "    # load train & test covariate data matrices\n",
        "    X_tr = np.loadtxt(os.path.join(roi_dir, 'cov_tr.txt'))\n",
        "    X_te = np.loadtxt(os.path.join(roi_dir, 'cov_te.txt'))\n",
        "    # add intercept column \n",
        "    X_tr = np.concatenate((X_tr, np.ones((X_tr.shape[0],1))), axis=1)\n",
        "    X_te = np.concatenate((X_te, np.ones((X_te.shape[0],1))), axis=1)\n",
        "    np.savetxt(os.path.join(roi_dir, 'cov_int_tr.txt'), X_tr)\n",
        "    np.savetxt(os.path.join(roi_dir, 'cov_int_te.txt'), X_te)\n",
        "    \n",
        "    # create Bspline basis set \n",
        "    Phi = np.array([B(i) for i in X_tr[:,0]])\n",
        "    Phis = np.array([B(i) for i in X_te[:,0]])\n",
        "    X_tr = np.concatenate((X_tr, Phi), axis=1)\n",
        "    X_te = np.concatenate((X_te, Phis), axis=1)\n",
        "    np.savetxt(os.path.join(roi_dir, 'cov_bspline_tr.txt'), X_tr)\n",
        "    np.savetxt(os.path.join(roi_dir, 'cov_bspline_te.txt'), X_te)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NbFaO7pWPxQo"
      },
      "source": [
        "## Estimate normative model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdZrf78vPxQo"
      },
      "source": [
        "### Step 10.\t\n",
        "\n",
        "Set up a variable (`data_dir`) that specifies the path to the ROI directories that were created in Step 7. Initiate two empty pandas data frames where the evaluation metrics are the column names, as follows; one will be used for overall test set evaluation (`blr_metrics`) and one will be used for site-specific test set evaluation (`blr_site_metrics`). After the normative model has been estimated, these data frames will be saved as individual csv files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 165,
      "metadata": {
        "id": "EJrly1KQPxQo"
      },
      "outputs": [],
      "source": [
        "# Create pandas dataframes with header names to save out the overall and per-site model evaluation metrics\n",
        "blr_metrics = pd.DataFrame(columns = ['ROI', 'MSLL', 'EV', 'SMSE', 'RMSE', 'Rho'])\n",
        "blr_site_metrics = pd.DataFrame(columns = ['ROI', 'site', 'MSLL', 'EV', 'SMSE', 'RMSE', 'Rho'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGPLrct5PxQo"
      },
      "source": [
        "### Step 11.\t\n",
        "\n",
        "Estimate the normative models using a for loop to iterate over brain regions. An important consideration is whether to re-scale or standardize the covariates or responses. Whilst this generally only has a minor effect on the final model accuracy, it has implications for the interpretation of models and how they are configured. If the covariates and responses are both standardized (`standardize = True`), the model will return standardized coefficients. If (as in this case) the response variables are not standardized (`standardized = False`), then the scaling both covariates and responses will be reflected in the estimated coefficients. Also, under the linear modeling approach employed here, if the coefficients are unstandardized and do not have a zero mean, it is necessary to add an intercept column to the design matrix (this is done above in step 9 (B-spline)). The estimate function uses a few specific arguments that are worthy of commenting on: \n",
        "\n",
        "    - alg = 'blr': specifies we should use Bayesian Linear Regression.  \n",
        "    - optimizer = 'powell': use Powell's derivative-free optimization method (faster in this case than L-BFGS) \n",
        "    - savemodel = False: do not write out the final estimated model to disk \n",
        "    - saveoutput = False: return the outputs directly rather than writing them to disk\n",
        "    - standardize = False: Do not standardize the covariates or response variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OL-Y-1WVPxQo"
      },
      "source": [
        "**CRITICAL STEP:** This code fragment will loop through each region of interest in the `roi_ids` list (created in step 4) using Bayesian Linear Regression and evaluate the model on the independent test set. In principle, we could estimate the normative models on the whole data matrix at once (e.g., with the response variables stored in a `n_subjects` by `n_brain_measures` NumPy array or a text file instead of saved out into separate directories). However, running the models iteratively gives some extra flexibility in that it does not require that the included subjects are the same for each of the brain measures. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 166,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFipXY_MPxQp",
        "outputId": "04e1c817-176d-47e5-c146-123a95e875de"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Running ROI: lh_MeanThickness_thickness\n",
            "Processing data in /home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/tutorials/BLR_protocol/ROI_models/lh_MeanThickness_thickness/resp_tr.txt\n",
            "Estimating model  1 of 1\n",
            "configuring BLR ( order 1 )\n",
            "Using default hyperparameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adimitrijevic/miniconda3/lib/python3.9/site-packages/pcntoolkit/model/bayesreg.py:187: LinAlgWarning: Ill-conditioned matrix (rcond=1.14578e-18): result may not be accurate.\n",
            "  invAXt = linalg.solve(self.A, X.T, check_finite=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: -1162.792820\n",
            "         Iterations: 2\n",
            "         Function evaluations: 47\n",
            "Saving model meta-data...\n",
            "Evaluating the model ...\n",
            "Running ROI: rh_MeanThickness_thickness\n",
            "Processing data in /home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/tutorials/BLR_protocol/ROI_models/rh_MeanThickness_thickness/resp_tr.txt\n",
            "Estimating model  1 of 1\n",
            "configuring BLR ( order 1 )\n",
            "Using default hyperparameters\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/adimitrijevic/miniconda3/lib/python3.9/site-packages/pcntoolkit/model/bayesreg.py:187: LinAlgWarning: Ill-conditioned matrix (rcond=4.21375e-19): result may not be accurate.\n",
            "  invAXt = linalg.solve(self.A, X.T, check_finite=False)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimization terminated successfully.\n",
            "         Current function value: -1187.621858\n",
            "         Iterations: 2\n",
            "         Function evaluations: 47\n",
            "Saving model meta-data...\n",
            "Evaluating the model ...\n",
            "Running ROI: lh_bankssts_thickness\n",
            "Processing data in /home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/tutorials/BLR_protocol/ROI_models/lh_bankssts_thickness/resp_tr.txt\n",
            "Estimating model  1 of 1\n",
            "configuring BLR ( order 1 )\n",
            "Using default hyperparameters\n",
            "Optimization terminated successfully.\n",
            "         Current function value: -578.945257\n",
            "         Iterations: 2\n",
            "         Function evaluations: 46\n",
            "Saving model meta-data...\n",
            "Evaluating the model ...\n",
            "Running ROI: lh_caudalanteriorcingulate_thickness\n",
            "Processing data in /home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/tutorials/BLR_protocol/ROI_models/lh_caudalanteriorcingulate_thickness/resp_tr.txt\n",
            "Estimating model  1 of 1\n",
            "configuring BLR ( order 1 )\n",
            "Using default hyperparameters\n",
            "Optimization terminated successfully.\n",
            "         Current function value: -235.509099\n",
            "         Iterations: 3\n",
            "         Function evaluations: 77\n",
            "Saving model meta-data...\n",
            "Evaluating the model ...\n",
            "Running ROI: lh_superiorfrontal_thickness\n",
            "Processing data in /home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/tutorials/BLR_protocol/ROI_models/lh_superiorfrontal_thickness/resp_tr.txt\n",
            "Estimating model  1 of 1\n",
            "configuring BLR ( order 1 )\n",
            "Using default hyperparameters\n",
            "Optimization terminated successfully.\n",
            "         Current function value: -716.547377\n",
            "         Iterations: 3\n",
            "         Function evaluations: 90\n",
            "Saving model meta-data...\n",
            "Evaluating the model ...\n",
            "Running ROI: rh_superiorfrontal_thickness\n",
            "Processing data in /home/adimitrijevic/dimitrijevic_project/PCNToolkitDemo/tutorials/BLR_protocol/ROI_models/rh_superiorfrontal_thickness/resp_tr.txt\n",
            "Estimating model  1 of 1\n",
            "configuring BLR ( order 1 )\n",
            "Using default hyperparameters\n",
            "Optimization terminated successfully.\n",
            "         Current function value: -730.639309\n",
            "         Iterations: 2\n",
            "         Function evaluations: 45\n",
            "Saving model meta-data...\n",
            "Evaluating the model ...\n"
          ]
        }
      ],
      "source": [
        "# Loop through ROIs\n",
        "for roi in roi_ids: \n",
        "    print('Running ROI:', roi)\n",
        "    roi_dir = os.path.join(data_dir, roi)\n",
        "    os.chdir(roi_dir)\n",
        "     \n",
        "    # configure the covariates to use. Change *_bspline_* to *_int_* to \n",
        "    cov_file_tr = os.path.join(roi_dir, 'cov_bspline_tr.txt')\n",
        "    cov_file_te = os.path.join(roi_dir, 'cov_bspline_te.txt')\n",
        "    \n",
        "    # load train & test response files\n",
        "    resp_file_tr = os.path.join(roi_dir, 'resp_tr.txt')\n",
        "    resp_file_te = os.path.join(roi_dir, 'resp_te.txt') \n",
        "    \n",
        "    # run a basic model\n",
        "    yhat_te, s2_te, nm, Z, metrics_te = estimate(cov_file_tr, \n",
        "                                                 resp_file_tr, \n",
        "                                                 testresp=resp_file_te, \n",
        "                                                 testcov=cov_file_te, \n",
        "                                                 alg = 'blr', \n",
        "                                                 optimizer = 'powell', \n",
        "                                                 savemodel = True, \n",
        "                                                 saveoutput = False,\n",
        "                                                 standardize = False)\n",
        "    # save metrics\n",
        "    blr_metrics.loc[len(blr_metrics)] = [roi, metrics_te['MSLL'][0], metrics_te['EXPV'][0], metrics_te['SMSE'][0], metrics_te['RMSE'][0], metrics_te['Rho'][0]]\n",
        "    \n",
        "    # Compute metrics per site in test set, save to pandas df\n",
        "    # load true test data\n",
        "    X_te = np.loadtxt(cov_file_te)\n",
        "    y_te = np.loadtxt(resp_file_te)\n",
        "    y_te = y_te[:, np.newaxis] # make sure it is a 2-d array\n",
        "    \n",
        "    # load training data (required to compute the MSLL)\n",
        "    y_tr = np.loadtxt(resp_file_tr)\n",
        "    y_tr = y_tr[:, np.newaxis]\n",
        "    \n",
        "    for num, site in enumerate(sites):     \n",
        "        y_mean_te_site = np.array([[np.mean(y_te[site])]])\n",
        "        y_var_te_site = np.array([[np.var(y_te[site])]])\n",
        "        yhat_mean_te_site = np.array([[np.mean(yhat_te[site])]])\n",
        "        yhat_var_te_site = np.array([[np.var(yhat_te[site])]])\n",
        "        \n",
        "        metrics_te_site = evaluate(y_te[site], yhat_te[site], s2_te[site], y_mean_te_site, y_var_te_site)\n",
        "        \n",
        "        site_name = site_names[num]\n",
        "        blr_site_metrics.loc[len(blr_site_metrics)] = [roi, site_names[num], metrics_te_site['MSLL'][0], metrics_te_site['EXPV'][0], metrics_te_site['SMSE'][0], metrics_te_site['RMSE'][0], metrics_te_site['Rho'][0]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc4NFsJBPxQp"
      },
      "source": [
        "# Evaluation & Interpretation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CAdaK2NPxQr"
      },
      "source": [
        "## Describe the normative model performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8syuqDxPxQr"
      },
      "source": [
        "### Step 12.\t\n",
        "\n",
        "In step 11, when we looped over each region of interest in the `roi_ids` list (created in step 4) and evaluated the normative model on the independent test set, it also computed the evaluation metrics such as the explained variance, mean standardized log-loss and Pearson correlation between true and predicted test responses. The evaluation metrics were calculated for the full test set and calculated separately for each scanning site. The metrics were saved out to a csv file. In this step we load the evaluation metrics into a panads data frame and use the describe function to show the range, mean, and standard deviation of each of the evaluation metrics. Table 2 shows how to interpret the ranges/directions of good model fit. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 167,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U3ncOg8FPxQs",
        "outputId": "94fb0f6a-f1ef-497c-b88f-73ee500d49ef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "count    6.000000\n",
            "mean     0.216747\n",
            "std      0.114371\n",
            "min      0.063284\n",
            "25%      0.161901\n",
            "50%      0.204015\n",
            "75%      0.264058\n",
            "max      0.397232\n",
            "Name: EV, dtype: float64\n",
            "count    6.000000\n",
            "mean    -0.131996\n",
            "std      0.080019\n",
            "min     -0.267055\n",
            "25%     -0.157321\n",
            "50%     -0.120775\n",
            "75%     -0.089765\n",
            "max     -0.034441\n",
            "Name: MSLL, dtype: float64\n",
            "count    6.000000\n",
            "mean     0.784798\n",
            "std      0.114679\n",
            "min      0.603410\n",
            "25%      0.736912\n",
            "50%      0.798426\n",
            "75%      0.841000\n",
            "max      0.936928\n",
            "Name: SMSE, dtype: float64\n",
            "count    6.000000\n",
            "mean     0.452088\n",
            "std      0.126840\n",
            "min      0.257838\n",
            "25%      0.403631\n",
            "50%      0.450319\n",
            "75%      0.513867\n",
            "max      0.630935\n",
            "Name: Rho, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Overall test set evaluation metrics\n",
        "print(blr_metrics['EV'].describe())\n",
        "print(blr_metrics['MSLL'].describe())\n",
        "print(blr_metrics['SMSE'].describe())\n",
        "print(blr_metrics['Rho'].describe())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZEs7Ej4-qGi"
      },
      "source": [
        "The deviation scores are output as a text file in separate folders. We want to summarize the deviation scores across all models estimates so we can organize them into a single file, and merge the deviation scores into the original data file. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kvh8npp-PxQt",
        "tags": []
      },
      "source": [
        "## Visualize normative model outputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXqJemw7PxQu"
      },
      "source": [
        "<div>\n",
        "<img src=\"https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/Figure4.png?raw=1\" width=\"1000\">\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "89GpR-mIPxQu"
      },
      "source": [
        "### Figure 4A viz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 168,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "cNeOX8RJPxQu",
        "outputId": "7f204afa-2ee3-452d-c0c3-b481eb97ac8e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "<function matplotlib.pyplot.show(close=None, block=None)>"
            ]
          },
          "execution_count": 168,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "text/plain": [
              "<Figure size 2280x1520 with 0 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 576x576 with 3 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "pd.set_option('display.max_rows', 500)\n",
        "pd.set_option('display.max_columns', 500)\n",
        "pd.set_option('display.width', 1000)\n",
        "\n",
        "def color_gradient(x=0.0, start=(0, 0, 0), stop=(1, 1, 1)):\n",
        "    r = np.interp(x, [0, 1], [start[0], stop[0]])\n",
        "    g = np.interp(x, [0, 1], [start[1], stop[1]])\n",
        "    b = np.interp(x, [0, 1], [start[2], stop[2]])\n",
        "    return r, g, b\n",
        "\n",
        "plt.figure(dpi=380)\n",
        "fig, axes = joypy.joyplot(blr_site_metrics, column=['EV'], overlap=2.5, by=\"site\", ylim='own', fill=True, figsize=(8,8)\n",
        "                          , legend=False, xlabels=True, ylabels=True, colormap=lambda x: color_gradient(x, start=(.08, .45, .8),stop=(.8, .34, .44))\n",
        "                          , alpha=0.6, linewidth=.5, linecolor='w', fade=True)\n",
        "plt.title('Test Set Explained Variance', fontsize=18, color='black', alpha=1)\n",
        "plt.xlabel('Explained Variance', fontsize=14, color='black', alpha=1)\n",
        "plt.ylabel('Site', fontsize=14, color='black', alpha=1)\n",
        "plt.show"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FTArq1PKPxQu"
      },
      "source": [
        "The code used to create the visualizations shown in Figure 4 panels B-F, can be found in this [notebook](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/visualizations.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTrvxDmGPxQu"
      },
      "source": [
        "## Post-Hoc analysis ideas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PlC69kfpPxQu"
      },
      "source": [
        "The code for running SVM classification and classical case vs. control t-testing on the outputs of normative modeling can be found in this [notebook](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/post_hoc_analysis.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fiATEEBdPxQv"
      },
      "source": [
        "The code for running other predictive models (regression, using the outputs of normative modeling as predictive features) can be found in this [notebook](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/other_predictive_models.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yapjr2CTPxQv"
      },
      "source": [
        "The code for transfering a pre-trained normative model to a new dataset can be found in this [notebook](https://github.com/predictive-clinical-neuroscience/PCNtoolkit-demo/blob/main/tutorials/BLR_protocol/transfer_pretrained_normative_models.ipynb)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "name": "BLR_normativemodel_protocol.ipynb",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "b479d5eb2472c6b73b88e48dbc8a55e76b4ed935cafd6fe54360f23cfe7b7e52"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
